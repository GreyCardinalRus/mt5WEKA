//+------------------------------------------------------------------+
//|                                                     MLP_func.mq4 |
//|                                                            Kadet |
//|                                              www.kadet.nsknet.ru |
//+------------------------------------------------------------------+
#property copyright "Kadet"
#property link      "www.kadet.nsknet.ru"

//--------------------------------------------
// MLPCreate0  // Print("Инициирована нейросеть.\nВ  качестве  функции активации скрытых слоев  используется  гиперболический  тангенс, \nвыходной  слой линейный. Без скрытых слоев.");
// MLPCreate1  // Print("Инициирована нейросеть.\nВ  качестве  функции активации скрытых слоев  используется  гиперболический  тангенс, \nвыходной  слой линейный. С одним скрытым слоем.");
// MLPCreate2  // Print("Инициирована нейросеть.\nВ  качестве  функции активации скрытых слоев  используется  гиперболический  тангенс, \nвыходной  слой линейный. С двумя скрытыми слоями.");

//--------------------------------------------
// MLPCreateB0 // Print("Инициирована нейросеть.\nВ качестве  функции активации скрытых слоев  используется  гиперболический  тангенс. \nДиапазон значений выходного слоя имеет вид: (B, +INF) если D>=0, или (-INF, B), если D<0. Без скрытых слоев.");
// MLPCreateB1 // Print("Инициирована нейросеть.\nВ качестве  функции активации скрытых слоев  используется  гиперболический  тангенс. \nДиапазон значений выходного слоя имеет вид: (B, +INF) если D>=0, или (-INF, B), если D<0. С одним скрытым слоем.");
// MLPCreateB2 // Print("Инициирована нейросеть.\nВ качестве  функции активации скрытых слоев  используется  гиперболический  тангенс. \nДиапазон значений выходного слоя имеет вид: (B, +INF) если D>=0, или (-INF, B), если D<0. С двумя скрытыми слоями.");

//--------------------------------------------
// MLPCreateR0 // Print("Инициирована нейросеть.\nВ качестве  функции активации скрытых слоев  используется  гиперболический  тангенс. \nДиапазон значений выходного слоя равен [A,B] (используется гиперболический тангенс в сочетании со сдвигом/машстабированием). Без скрытых слоев.");
// MLPCreateR1 // Print("Инициирована нейросеть.\nВ качестве  функции активации скрытых слоев  используется  гиперболический  тангенс. \nДиапазон значений выходного слоя равен [A,B] (используется гиперболический тангенс в сочетании со сдвигом/машстабированием). С одним скрытым слоем.");
// MLPCreateR2 // Print("Инициирована нейросеть.\nВ качестве  функции активации скрытых слоев  используется  гиперболический  тангенс. \nДиапазон значений выходного слоя равен [A,B] (используется гиперболический тангенс в сочетании со сдвигом/машстабированием). С двумя скрытыми слоями.");

//--------------------------------------------
// Нейронная сеть-классификатор. 
// MLPCreateC0 // Print("Инициирована нейросеть.\nНейронная сеть-классификатор. Без скрытых слоев.");
// MLPCreateC1 // Print("Инициирована нейросеть.\nНейронная сеть-классификатор. С одним скрытым слоем.");
// MLPCreateC2 // Print("Инициирована нейросеть.\nНейронная сеть-классификатор. С двумя скрытыми слоями.");


//Глобальные константы
#define NFieldWidth 4.0
#define ChunkSize   32.0
#define FTOL 0.0001
#define MAXFEV 20.0
#define GTOL 0.9
#define MinWStep 0.0001
#define StoppingBufSize 20
#define StoppingMin 50.0
#define StoppingP 0.01

//Глобальные константы
static double XTOL,STPMIN, STPMAX, MinGVal, MachineEpsilon, 
               MaxRealNumber, MinRealNumber, BigNumber, SmallNumber;

//   MachineEpsilon = 5*MathPow(10.0,-16);   // 5E-16
//   MaxRealNumber  = MathPow(10.0,300);     // 1E+300;
//   MinRealNumber  = MathPow(10.0,-300);    // 1*e-300;
//   BigNumber      = MathPow(10.0,70);      // 1*e+70;
//   SmallNumber    = MathPow(10.0,-70);     // 1*e-70;

int t; 

//+------------------------------------------------------------------+
//| Описание массива MultiLayerPerceptron (тип переменных)           |
//| Обученная нейронная сеть                                         |
//+------------------------------------------------------------------+

double MultiLayerPerceptron [10,11,300,40];     // Прототип массива Public Type MultiLayerPerceptron
                                                // MultiLayerPerceptron [x,y,z,n]:
                                                // Где x -  название переменной типа MultiLayerPerceptron
                                                //          выраженное в числовом эквиваленте типа int
                                                //          name_var = 0, 1, 2, ... n
                                                //     y -  название переменной из типа MultiLayerPerceptron (описаны ниже)
                                                //     z -  размерность массивов внутри типа MultiLayerPerceptron (берётся по максимальному)
int
    StructInfo    = 0,                          // int StructInfo[];       - МАССИВ
    Weights       = 1,                          // double Weights[];       - МАССИВ    - весовые коэффициенты
    ColumnMeans   = 2,                          // double ColumnMeans[];   - МАССИВ
    ColumnSigmas  = 3,                          // double ColumnSigmas[];  - МАССИВ
    Neurons       = 4,                          // double Neurons[];       - МАССИВ
    DFDNET        = 5,                          // double DFDNET[];        - МАССИВ
    DError        = 6,                          // double DError[];        - МАССИВ
    MLP_X         = 7,                          // double X[];             - МАССИВ
    MLP_Y         = 8,                          // double Y[];             - МАССИВ
    Chunks        = 9,                          // double Chunks[];        - МАССИВ - двухмерный
    NWBuf         = 10;                         // double NWBuf[];         - МАССИВ
                                                // End Type

//+------------------------------------------------------------------+
//| Описание массива MLPTrainingReport (тип переменных)              |
//+------------------------------------------------------------------+
double MLPTrainingReport[10,4];                 // Прототип массива Public Type MLPTrainingReport
                                                // MLPTrainingReport [x,y]:
                                                // Где x -  название переменной типа MLPTrainingReport
                                                //          выраженное в числовом эквиваленте типа int
                                                //          name_var = 0, 1, 2, ... n
                                                //     y -  название переменной из типа MLPTrainingReport (описаны ниже)
int
    RMS_Error     = 0,                          // double RMSError ;
    RelCL_SError  = 1,                          // double RelCLSError ;
    N_Error       = 2,                          // double NError ;
    Epoch_Count   = 3;                          // int EpochCount;
                                                // End Type
//+------------------------------------------------------------------+
//| Описание массива LBFGSState (тип переменных)                     |
//+------------------------------------------------------------------+
double LBFGSState [10,51,330,330];              // Прототип массива Public Type LBFGSState
                                                // LBFGSState [x,y,z,n]:
                                                // Где x -  название переменной типа LBFGSState
                                                //          выраженное в числовом эквиваленте типа int
                                                //          name_var = 0, 1, 2, ... n
                                                //     y -  название переменной из типа LBFGSState (описаны ниже)
                                                //     z -  размерность массивов внутри типа LBFGSState (берётся по максимальному)
int
    LBF_N      = 0,                             // N int
    LBF_M      = 1,                             // M int
    EpsG       = 2,                             // EpsG double
    EpsF       = 3,                             // EpsF double
    EpsX       = 4,                             // EpsX double
    MaxIts     = 5,                             // MaxIts int
    Flags      = 6,                             // Flags int
    LBF_NFEV   = 7,                             // NFEV int
    MCStage    = 8,                             // MCStage int
    LBF_K      = 9,                             // K int
    LBF_Q      = 10,                            // Q int
    LBF_P      = 11,                            // P int
    Rho        = 12,                            // Rho() double      - МАССИВ
    LBF_Y      = 13,                            // Y() double        - МАССИВ - двумерный
    LBF_S      = 14,                            // S() double        - МАССИВ - двумерный
    Theta      = 15,                            // Theta() double    - МАССИВ
    LBF_D      = 16,                            // D() double        - МАССИВ
    Stp        = 17,                            // Stp double
    WORK       = 18,                            // WORK() double     - МАССИВ
    FOld       = 19,                            // FOld double
    GammaK     = 20,                            // GammaK double
    LBF_Stage  = 21,                            // Stage int
    LBF_X      = 22,                            // X() double        - МАССИВ
    LBF_F      = 23,                            // F double
    LBF_G      = 24,                            // G() double        - МАССИВ
    XUpdated   = 25,                            // XUpdated bool
    LBF_BRACKT = 26,                            // BRACKT bool
    STAGE1     = 27,                            // STAGE1 bool
    INFOC      = 28,                            // INFOC int
    DG         = 29,                            // DG double
    DGM        = 30,                            // DGM double
    DGINIT     = 31,                            // DGINIT double
    DGTEST     = 32,                            // DGTEST double
    DGX        = 33,                            // DGX double
    DGXM       = 34,                            // DGXM double
    DGY        = 35,                            // DGY double
    DGYM       = 36,                            // DGYM double
    FINIT      = 37,                            // FINIT double
    FTEST1     = 38,                            // FTEST1 double
    FM         = 39,                            // FM double
    LBF_FX     = 40,                            // FX double
    FXM        = 41,                            // FXM double
    LBF_FY     = 42,                            // FY double
    FYM        = 43,                            // FYM double
    LBF_STX    = 44,                            // STX double
    LBF_STY    = 45,                            // STY double
    LBF_STMIN  = 46,                            // STMIN double
    LBF_STMAX  = 47,                            // STMAX double
    WIDTH      = 48,                            // WIDTH double
    WIDTH1     = 49,                            // WIDTH1 double
    XTRAPF     = 50;                            // XTRAPF double
                                                // End Type
//+------------------------------------------------------------------+
//| Описание массива LBFGSReport (тип переменных)                    |
//+------------------------------------------------------------------+
double LBFGSReport[10,3];                       // Прототип массива Public Type LBFGSReport
                                                // LBFGSReport [x,y]:
                                                // Где x -  название переменной типа LBFGSReport
                                                //          выраженное в числовом эквиваленте типа int
                                                //          name_var = 0, 1, 2, ... n
                                                //     y -  название переменной из типа LBFGSReport (описаны ниже)
int
    IterationsCount  = 0,                       // IterationsCount int
    Rep_NFEV         = 1,                       // NFEV int
    TerminationType  = 2;                       // TerminationType int
                                                // End Type
//-----------------------------------------------------------------------------------------------------------------

//+------------------------------------------------------------------+
//| Функция нахождения тангенса                                      |
//+------------------------------------------------------------------+
double TanH(double Z) {
//--------------------------------------------
   double Rezult, A1, A2;
//--------------------------------------------
   A1 = (MathExp(Z)-MathExp(-Z));
   A2 = (MathExp(Z)+MathExp(-Z));
   Rezult = (MathExp(Z)-MathExp(-Z)) / (MathExp(Z)+MathExp(-Z));
//--------------------------------------------
   return(Rezult);
}

//+------------------------------------------------------------------+
//| Знак числа числа                                                 |
//+------------------------------------------------------------------+
double Sgn(double Z ) {
    return( Z/MathAbs(Z) );
}

//+------------------------------------------------------------------+
//| Выдаёт случайное число о или 1                                   |
//+------------------------------------------------------------------+
int RandomInteger(int Z ) {
    return( MathRound(Z*RandomReal()) );
}


//+------------------------------------------------------------------+
//| Выдаёт случайное вещественное число от 0 до 1                    |
//+------------------------------------------------------------------+
int RandomReal() {
    return( MathRand()/32767.0 );
}



//+------------------------------------------------------------------+
//|  Получение информации о нейронной сети                           |
//|                                                                  |
//|  Входные параметры:                                              |
//|      Network     -   нейронная сеть                              |
//|                                                                  |
//|  Выходные параметры:                                             |
//|      NIn         -   число входов                                |
//|      NOut        -   число выходов                               |
//|      WCount      -   число весовых/пороговых коэффициентов       |
//|                                                                  |
//|    -- ALGLIB --                                                  |
//|       Copyright 04.11.2007 by Bochkanov Sergey                   |
//+------------------------------------------------------------------+
void MLPProperties(  int Network,      // структура НС                           - (входной параметр)
                     int& NIn,         // число входных данных                   - (выходной параметр)
                     int& NOut,        // число выходных данных                  - (выходной параметр)
                     int& WCount){     // число весовых/пороговых коэффициентов  - (выходной параметр)
//--------------------------------------------
   NIn = MultiLayerPerceptron[Network,StructInfo,1,0];              // число входных данных
   NOut = MultiLayerPerceptron[Network,StructInfo,2,0];             // число выходных данных
   WCount = MultiLayerPerceptron[Network,StructInfo,4,0];           // число весовых/пороговых коэффициентов
//--------------------------------------------
}

//+---------------------------------------------------------------------------+
//|Оценка способности сети к обобщению с использованием кросс-валидации       |
//|                                                                           |
//|ВХОДНЫЕ ПАРАМЕТРЫ:                                                         |
//|    Network     -   нейронная сеть с инициализированной геометрией         |
//|    XY          -   обучающее множество.                                   |     DB[,]
//|                    массив размером [0..SSize-1, 0..NIn+NOut-1].           |     SSize = величина истории, кол-во баров. (16 000)
//|                    первые   NIn   столбцов   содержат   входные   данные, |     NIn=9  - данные осциляторов
//|                    последующие NOut столбцов  содержат желаемые  значения |     NOut=1 - результат - тренд.
//|                    выходов сети.                                          |
//|    SSize       -   размер обучающего множества                            |     SSize = величина истории, кол-во баров. (16 000)
//|    Decay       -   константа  weight  decay,  >=0.   К   функции   ошибки |
//|                    добавляется слагаемое вида Decay*SSize*||Weights||     |
//|    Restarts    -   число стартов алгоритма из случайной позиции, >0.      |
//|                    рестарты считаются для каждого разбиения в отдельности,|
//|                    т.е. общее число рестартов будет Restarts*FoldsCount.  |
//|    NError      -   True, если используется естественная функция ошибка.   |
//|                    False, если используется полусумма квадратов.          |
//|    FoldsCount  -   число разбиений множества, 2<=FoldsCount<=SSize.       |
//|                    Рекомендуемое значение: 2..10.                         |
//|                                                                           |
//|ВЫХОДНЫЕ ПАРАМЕТРЫ:                                                        |
//|    RMSError    -   оценка среднеквадратичной ошибки сети                  |
//|    RelCLSERror -   оценка относительной ошибки классификации (процента    |
//|                    неверно классифицируемых случаев, в диапазоне 0..1).   |
//|                                                                           |
//|                                                                           |
//|  -- ALGLIB --                                                             |
//|     Copyright 09.12.2007 by Bochkanov Sergey                              |
//|                                                                           |
//+---------------------------------------------------------------------------+

void MLPKFoldCV( int Network,             // - структура НС                                           - (входной параметр)
                  double& XY1[][],          // - входное множество (кол-во опытов)                      - (входной параметр)
                  int SSize,              // - размер входного множества                              - (входной параметр)
                  double Decay,           // - константа  weight  decay,  >=0.                        - (входной параметр)
                  int Restarts,           // - число стартов алгоритма из случайной позиции, >0.      - (входной параметр)
                  bool NError,            // - True, если используется естественная функция ошибка.   - (входной параметр)
//                                             False, если используется полусумма квадратов.
                  int FoldsCount,         // - число разбиений множества, 2<=FoldsCount<=SSize.       - (входной параметр)
//                                             Рекомендуемое значение: 2..10.

                  double& RMSError,        // - оценка среднеквадратичной ошибки сети                  - (выходной параметр)
                  double& RelCLSError) {   // - оценка относительной ошибки классификации              - (выходной параметр)
//--------------------------------------------
   int NIn, NOut, WCount, Fold, Network2, TSSize, CVSSize, InternalRep, Folds[];
   double iCVSet[], LocalTemp[], TestSet[][100], CVSet[][100], XY[][40];
//--------------------------------------------
//   Print("Оценка способности сети к обобщению.");
//--------------------------------------------
// Чтение геометрии сети, проверка параметров
   MLPProperties(Network, NIn, NOut, WCount);
//--------------------------------------------
   MLPCopy(Network, Network2);
//--------------------------------------------
   ArrayCopy(XY, XY1);
//--------------------------------------------
// RСлучайный набор
   ArrayResize(LocalTemp,(NIn+NOut));
   ArrayResize(iCVSet,(NIn+NOut));
   MathSrand(1);
   for( int i=0; i<SSize-1; i++){
      int j = i+RandomInteger(SSize-i);
      if( j!=i ) {
         for( int i_=0; i_<NIn+NOut; i_++)
            LocalTemp[i_] = XY[i,i_];
            for( i_=0; i_<NIn+NOut; i_++)
               XY[i,i_] = (XY[j,i_]);
            for( i_=0; i_<NIn+NOut; i_++)
               XY[j,i_] = LocalTemp[i_];
      }
   }
//--------------------------------------------
// K-тая поперечная ратификация.
// Сначала, оценка ошибки обобщения
   ArrayResize(TestSet,SSize);     // ReDim TestSet(0 To SSize-1, 0 To NIn+NOut-1)
   ArrayResize(CVSet,SSize);       // ReDim CVSet(0 To SSize-1, 0 To NIn+NOut-1)
   ArrayResize(Folds,SSize);       // ReDim Folds(0 To SSize-1)
//--------------------------------------------
   for( i=0; i<SSize; i++)
      Folds[i] = i*FoldsCount/SSize;  // Folds(I) = I*FoldsCount\SSize
//--------------------------------------------
   RMSError = 0.0;
   RelCLSError = 0.0;
   for( Fold=0; Fold<FoldsCount; Fold++) {
//--------------------------------------------
// Отдельный набор
      TSSize = 0.0;
      CVSSize = 0.0;
      for( i=0; i<SSize; i++) {
         if( Folds[i]==Fold ) {
            for( i_=0; i_<NIn+NOut; i_++)
               TestSet[TSSize,i_] = XY[i,i_];
            TSSize = TSSize+1.0;
         } else {
            for( i_=0; i_<NIn+NOut; i_++)
               CVSet[CVSSize,i_] = (XY[i,i_]);
            CVSSize = CVSSize+1.0;
         }
      }
//--------------------------------------------
// Поезд на CV устанавке
      if( NError )
         MLPTrainNSimple(Network2,     // - нейронная сеть с инициализированной геометрией      - (входной параметр)
                        CVSet,         // - обучающее множество (DB[][])                        - (входной параметр)
                        CVSSize,       // - размер обучающего множества                         - (входной параметр)
                        Decay,         // - константа  weight  decay,  >=0.                     - (входной параметр)
                        Restarts,      // - число стартов алгоритма из случайной позиции, >0.   - (входной параметр)
                        
                        InternalRep);  // - отчет об обучении                                   - (выходной параметр)
      else
         MLPTrainSimple(Network2,      // - нейронная сеть с инициализированной геометрией      - (входной параметр)
                        CVSet,         // - обучающее множество (DB[][])                        - (входной параметр)
                        CVSSize,       // - размер обучающего множества                         - (входной параметр)
                        Decay,         // - константа  weight  decay,  >=0.                     - (входной параметр)
                        Restarts,      // - число стартов алгоритма из случайной позиции, >0.   - (входной параметр)
                        
                        InternalRep);  // - отчет об обучении                                   - (выходной параметр)
//--------------------------------------------
// Тогда, оцените ошибку, используя испытательный набор
        RMSError = RMSError+MLPError(Network2, TestSet, TSSize);
        RelCLSError = RelCLSError+MLPClsError(Network2, TestSet, TSSize);
   }
//--------------------------------------------
   RMSError = MathSqrt(2* RMSError/(NOut*SSize));
   RelCLSError = RelCLSError/SSize;
//--------------------------------------------
}


//+--------------------------------------------------------------------------------+
//|                                                                                |
//|  Обучение многослойного перспептрона (простой вариант, с минимумом опций)      |
//|                                                                                |
//|  Подпрограмма осуществляет обучение нейронной сети с указанным  количеством    |
//|  рестартов из случайных позиций. Для минимизации ошибки используется L-BFGS    |
//|  алгоритм.                                                                     |
//|                                                                                |
//|  ВХОДНЫЕ ПАРАМЕТРЫ:                                                            |
//|      Network     -   нейронная сеть с инициализированной геометрией            |
//|      XY,         -   обучающее множество.                                      |     DB[,]
//|                      массив размером [0..SSize-1, 0..NIn+NOut-1].              |     SSize = величина истории, кол-во баров. (16 000)
//|                      первые   NIn   столбцов   содержат   входные   данные,    |     NIn=9  - данные осциляторов
//|                      последующие NOut столбцов  содержат желаемые  значения    |     NOut=1 - результат - тренд.
//|                      выходов сети.                                             |
//|      SSize       -   размер обучающего множества                               |     SSize = величина истории, кол-во баров. (16 000)
//|      Decay       -   константа  weight  decay,  >=0.   К   функции   ошибки    |
//|                      добавляется слагаемое вида Decay*||Weights||^2            |
//|      Restarts    -   число стартов алгоритма из случайной позиции, >0.         |
//|                                                                                | 
//|  ВЫХОДНЫЕ ПАРАМЕТРЫ:                                                           |
//|      Network     -   обученная нейронная сеть.  В  ходе обучения предыдущее    |
//|                      состояние сети необратимо уничтожается,  в  том  числе    |
//|                      уничтожаются результаты предыдущих попыток обучения.      |
//|      Rep         -   отчет об обучении                                         |
//|                                                                                |
//|  ОТЧЕТ ОБ ОБУЧЕНИИ:                                                            |
//|                                                                                |
//|  Поля структуры MLPTrainingReport:                                             |
//|      * Поле  NError  содержит  общую  ошибку на обучающем множестве, которая   |
//|        рассчитывается по формуле  SUM(0.5*|result(i)-desired(i)|^2) для сети   |
//|        без softmax-нормализации выходов, или как кросс-энтропия  -  для сети   |
//|        с нормализацией выходов.                                                |
//|      * Поле  RMSError  содержит  среднеквадратичную  ошибку   на   обучающем   |
//|        множестве                                                               |
//|      * Поле  RelCLSError  содержит  относительную  ошибку  классификации  на   |
//|        обучающем множестве,  которая  рассчитывается  как   отношение  числа   |
//|        неверно классифицированных образов к общему числу образов.              |
//|      * EpochCount - количество эпох, прошедших в ходе обучения                 |
//|                                                                                |
//|    -- ALGLIB --                                                                |
//|       Copyright 09.12.2007 by Bochkanov Sergey                                 |
//+--------------------------------------------------------------------------------+

void MLPTrainSimple( int Network,   // - нейронная сеть с инициализированной геометрией      - (входной параметр)
                     double& XY[],  // - обучающее множество (DB[][])                        - (входной параметр)
                     int SSize,     // - размер обучающего множества                         - (входной параметр)
                     double Decay,  // - константа  weight  decay,  >=0.                     - (входной параметр)
                     int Restarts,  // - число стартов алгоритма из случайной позиции, >0.   - (входной параметр)
                     
                     int& Rep) {     // - отчет об обучении                                   - (выходной параметр)

    MLPTrainInternal(Network,       // - нейронная сеть с инициализированной геометрией      - (входной параметр)
                     XY,            // - обучающее множество (DB[][])                        - (входной параметр)
                     SSize,         // - размер обучающего множества                         - (входной параметр)
                     Decay,         // - константа  weight  decay,  >=0.                     - (входной параметр)
                     Restarts,      // - число стартов алгоритма из случайной позиции, >0.   - (входной параметр)
                     
                     Rep,           // - отчет об обучении                                   - (выходной параметр)
                     False, 
                     MinWStep, 
                     StoppingP);
}

//+--------------------------------------------------------------------------------+
//|  Аналог функции MLPTrainSimple,  использующий естественную  функцию  ошибки    |
//|  (сумму квадратов для обычной сети,  кросс-энтропию  для  сети  с  softmax-    |
//|  нормализацией выходов).                                                       |
//|                                                                                |
//|    -- ALGLIB --                                                                |
//|       Copyright 28.01.2008 by Bochkanov Sergey                                 |
//+--------------------------------------------------------------------------------+

void MLPTrainNSimple(int Network,   // - нейронная сеть с инициализированной геометрией      - (входной параметр)
                     double& XY[],  // - обучающее множество (DB[][])                        - (входной параметр)
                     int SSize,     // - размер обучающего множества                         - (входной параметр)
                     double Decay,  // - константа  weight  decay,  >=0.                     - (входной параметр)
                     int Restarts,  // - число стартов алгоритма из случайной позиции, >0.   - (входной параметр)
                     
                     int& Rep ) {    // - отчет об обучении                                   - (выходной параметр)

    MLPTrainInternal(Network,       // - нейронная сеть с инициализированной геометрией      - (входной параметр)
                     XY,            // - обучающее множество (DB[][])                        - (входной параметр)
                     SSize,         // - размер обучающего множества                         - (входной параметр)
                     Decay,         // - константа  weight  decay,  >=0.                     - (входной параметр)
                     Restarts,      // - число стартов алгоритма из случайной позиции, >0.   - (входной параметр)
                     
                     Rep,           // - отчет об обучении                                   - (выходной параметр)
                     True, 
                     MinWStep, 
                     StoppingP);
}


//+---------------------------------------------------------------------------------------------+
//| Внутренняя подпрограмма для обучения нейронной сети.                                        |
//|                                                                                             |
//|  Параметры то же самое как в MLPTrainSimple за исключением Перезапусков (повторений).       |
//|  Повторений > 0 рассматривают, тот же самый путь, но Restarts=0 возможно также.             |
//|                                                                                             |
//|  Restarts=0 означает, что алгоритм "переобучает" уже обучаемую сеть:                        |
//|  * алгоритм не рандомизирует веса, только делать один проход из данного положения           |
//|  * алгоритм не изменяет standartizator                                                      |
//|                                                                                             |
//|  -- ALGLIB --                                                                               |
//|     Copyright 28.01.2008 by Bochkanov Sergey                                                |
//+---------------------------------------------------------------------------------------------+
void MLPTrainInternal(  int Network,            // - структура НС                                        - (входной параметр)
                        double XY[][],          // - входное множество                                   - (входной параметр)
                        int SSize,              // - размер входного множества                           - (входной параметр)
                        double Decay,           // - константа  weight  decay,  >=0.                     - (входной параметр)
                        int Restarts,           // - число стартов алгоритма из случайной позиции, >0.   - (входной параметр)
                        
                        int& Rep,               // - 
                        bool NaturalErrorFunc,  // - 
                        double WStep,           // - 
                        double FStep){          // - 
//--------------------------------------------
//   Print("Процедура обучения.");
//--------------------------------------------
   int NIn, NOut, WCount, Info, Pass, InternalRep, State;
   double EBest, E, V, Tmp0, Tmp1, F1, F2, Mean, Variance, Ft, Gt[], W[], WBest[], LocalTemp[], FBuf[];
   MinGVal = MathPow((5.0*MathPow(10.0,-16)),2);
//--------------------------------------------
// Флаги разбора, прочитайте геометрию сети, проверьте параметры
   MLPProperties(Network, NIn, NOut, WCount);
//--------------------------------------------
// Подготовите лучшее решение
   if( Restarts>0 ) 
      MLPInitPreprocessor(Network, XY, SSize);
//--------------------------------------------
// Подготовите лучшее решение
   ArrayResize(W,WCount);
   ArrayResize(WBest,WCount);
   ArrayInitialize(WBest,0);
   EBest = MathPow(10.0,300);    // MaxRealNumber;
//--------------------------------------------
// Многократные запуски
   ArrayResize(FBuf,StoppingBufSize);
   ArrayInitialize(FBuf,0);
//--------------------------------------------
   MLPTrainingReport[Rep,Epoch_Count] = 0.0;             // Rep.Epoch_Count = 0
   MLPTrainingReport[Rep,RMS_Error] = MathPow(10.0,300); // Rep.RMS_Error = MaxRealNumber
   MLPTrainingReport[Rep,RelCL_SError] = 1.0;            // Rep.RelCL_SError = 1
   MLPTrainingReport[Rep,N_Error] = MathPow(10.0,300);   // Rep.N_Error = MaxRealNumber
   Pass = 1.0;
//--------------------------------------------
   while (True){
// Процесс
      if( Restarts>0 ) MLPRandomize(Network);
//--------------------------------------------
      for( int j=0; j<WCount; j++)
         W[j] = MultiLayerPerceptron[Network,Weights,j,0]; // W(i_) = Network.Weights(i_)
//--------------------------------------------
      MinLBFGS(WCount, MathMin(WCount,50), W, MinGVal, 0.0, WStep, 0, 0, State, InternalRep);
//--------------------------------------------
      while (LBFGSState[State,LBF_Stage,0,0]!=0) {
// Установка конечного буфера
         if( LBFGSState[State,XUpdated,0,0]>0 && FStep!=0 ) {
            for( int i=0; i<=StoppingBufSize-1; i++ ) 
               FBuf[i+1] = FBuf[i];
            FBuf[0] = LBFGSState[State,FOld,0,0];
// Тест, останавливающий критерий.
            if( LBFGSReport[InternalRep,IterationsCount]>=StoppingMin ) {
               F1 = FBuf[StoppingBufSize-1];
               F2 = FBuf[0];
               if( F1-F2 <= F1*FStep ) break;
            }
         }
//--------------------------------------------
// Градиент
         for( j=0; j<WCount; j++)
            MultiLayerPerceptron[Network,Weights,j,0] = LBFGSState[State,LBF_X,j,0]; // Network.Weights(i_) = State.X(i_)
//--------------------------------------------
            if( NaturalErrorFunc )
               MLPGradNBatch(Network,XY,SSize,Ft,Gt);       // (LBFGSState[State,LBF_F,0,0]),(LBFGSState[State,LBF_G,0,0]));
            else
               MLPGradBatch(Network,XY,SSize,Ft,Gt);       // (LBFGSState[State,LBF_F,0,0]),(LBFGSState[State,LBF_G,0,0]));
//--------------------------------------------
            LBFGSState[State,LBF_F,0,0] = Ft;
            for( i=0; i<WCount; i++)
               LBFGSState[State,LBF_G,i,0] = Gt[i];
//--------------------------------------------
// распад веса
            V = 0.0;
            for( j=0; j<WCount; j++)
               V += MathPow(MultiLayerPerceptron[Network,Weights,j,0],2);
            LBFGSState[State,LBF_F,0,0] += 0.5*Decay*V;
//--------------------------------------------
            for( j=0; j<WCount; j++)
               LBFGSState[State,LBF_G,j,0] += Decay*MultiLayerPerceptron[Network,Weights,j,0];
//--------------------------------------------
// Следующая итерация
            MLPTrainingReport[Rep,Epoch_Count] += 1.0;
            MinLBFGSIteration(W, State, InternalRep);
      } // Loop
//--------------------------------------------
      for( j=0; j<WCount; j++)
            MultiLayerPerceptron[Network,Weights,j,0] = W[j];
//--------------------------------------------
// Сравнитесь с лучшим результатом
        if( NaturalErrorFunc ){
            E = MLPErrorN(Network, XY, SSize);
        } else
            E = MLPError(Network, XY, SSize);
//--------------------------------------------
      if( E<EBest ) {
         for( j=0; j<WCount; j++)
            WBest[j] = MultiLayerPerceptron[Network,Weights,j,0];
         EBest = E;
         MLPTrainingReport[Rep,RMS_Error] = MathSqrt(MLPError(Network, XY, SSize)*2/(NOut*SSize));
         MLPTrainingReport[Rep,RelCL_SError] = MLPClsError(Network, XY, SSize)/SSize;
         MLPTrainingReport[Rep,N_Error] = MLPErrorN(Network, XY, SSize);
      }
//--------------------------------------------
// Следующая итерация
      Pass++;
      if( Pass>Restarts ) break;
   } // Loop
//--------------------------------------------
// Лучшая найденная сеть и ее параметры
   for( j=0; j<=WCount; j++)
      MultiLayerPerceptron[Network,Weights,j,0] = WBest[j];
//--------------------------------------------
}




//+------------------------------------------------------------------------------+
//|  Инициализация предобработчика данных на основе выборки.                     |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      Network -   структура, задающая нейронную  сеть  с  инициализированной  |
//|                  геометрией.                                                 |
//|      XY      -   выборка.                                                    |
//|                  массив размером [0..SSize-1,0..NIn+NOut-1].                 |
//|                  первые   NIn   столбцов   содержат   входные   данные,      |
//|                  последующие NOut столбцов  содержат желаемые  значения      |
//|                  выходов сети.                                               |
//|      SSize   -   размер выборки                                              |
//|                                                                              |
//|  Выходные параметры:                                                         |
//|      Network -   нейронная сеть с той же геометрией, и инициализированным    |
//|                  предобработчиком данных                                     |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 30.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPInitPreprocessor(  int Network,      // - структура НС                               - (входной параметр)
                           double XY[][],     // - тестовое множество (результаты опытов)     - (входной параметр)
                           int SSize ){      // - размер тестового множества (кол-во опытов) - (входной параметр)
//--------------------------------------------
//   Print("Процедура инициализации.");
//--------------------------------------------
   int NIn, NOut, WCount, NTotal, IStart, Offs, NType;
   double S, Means[], Sigmas[];
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);
   NTotal = MultiLayerPerceptron[Network,StructInfo,3,0];
   IStart = MultiLayerPerceptron[Network,StructInfo,5,0];
//--------------------------------------------
   // Means/Sigmas
   ArrayResize(Means,NIn+NOut);                    // ReDim Means(0 To NIn+NOut-1)
   ArrayResize(Sigmas,NIn+NOut);                   // ReDim Sigmas(0 To NIn+NOut-1)
   for( int j=0; j<NIn+NOut; j++){
      Means[j] = 0.0;
      for( int i=0; i<SSize; i++)
         Means[j] = Means[j]+XY[i,j];
      Means[j] /= SSize;
      Sigmas[j] = 0.0;
      for( i=0; i<SSize; i++)
         Sigmas[j] += MathPow((XY[i,j]-Means[j]),2);
      Sigmas[j] = MathSqrt(Sigmas[j]/SSize);
   } // for J
//--------------------------------------------
// Inputs
   for( i=0; i<NIn; i++){
      MultiLayerPerceptron[Network,ColumnMeans,i,0] = Means[i];
      MultiLayerPerceptron[Network,ColumnSigmas,i,0] = Sigmas[i];
      if( MultiLayerPerceptron[Network,ColumnSigmas,i,0]==0 )
            MultiLayerPerceptron[Network,ColumnSigmas,i,0] = 1.0;
   } // for I
//--------------------------------------------
// Outputs
   for( i=0; i<NOut; i++){
      Offs = IStart+(NTotal-NOut+i)*NFieldWidth;
      NType = MultiLayerPerceptron[Network,StructInfo,Offs,0];
// Линейный выход
      if( NType==0 ) {
         MultiLayerPerceptron[Network,ColumnMeans,(NIn+i),0] = Means[(NIn+i)];
         MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0] = Sigmas[(NIn+i)];
         if( MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0]==0 )
            MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0] = 1.0;
      }
// Ограниченные продукции (полуинтервал)
      if( NType==3 ) {
         S = Means[NIn+i]-MultiLayerPerceptron[Network,ColumnMeans,(NIn+i),0];
         if( S==0 )
            S = MathSqrt(MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0]);
         if( S==0 )
            S = 1.0;
         MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0] = 
                  Sgn(MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0])*MathAbs(S);
         if( MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0]==0 )
            MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0] = 1.0;
      }
   } // for I
}
//--------------------------------------------


//+------------------------------------------------------------------------+
//|                                                                        |
//| Рандомизация MultiLayerPerceptron без изменения структуры связей.      |
//|                                                                        |
//| Подпрограмма заполняет веса малыми случайными значениями.              |
//| Состояние стандартизатора входов и прочие свойства не меняются.        |
//|                                                                        |
//| Входные параметры:                                                     |
//|     Network         -   инициализированная обученная/необученная сеть  |
//|                                                                        |
//| Выходные параметры:                                                    |
//|     Network         -   рандомизированная сеть                         |
//|                                                                        |
//|   -- ALGLIB --                                                         |
//|      Copyright 06.11.2007 by Bochkanov Sergey                          |
//|                                                                        |
//+------------------------------------------------------------------------+
void MLPRandomize( int Network ) {        // входной параметр - структура НС
//--------------------------------------------
//   Print("Процедура рандомизации.");
//--------------------------------------------
   int NIn, NOut, WCount;
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);                             // по заданой структуре устанавливает значения - NIn, NOut, WCount
//--------------------------------------------
   MathSrand(1);
   for( int i=0; i<WCount; i++)
      MultiLayerPerceptron[Network,Weights,i,0] = NormalizeDouble((2.0*RandomReal()),8)-1.0;
//--------------------------------------------
}


//+---------------------------------------------------------------------------------+
//|           LIMITED MEMORY BFGS METHOD FOR LARGE SCALE OPTIMIZATION               |
//|       ОГРАНИЧЕННАЯ ПАМЯТЬ МЕТОД BFGS ДЛЯ КРУПНОМАСШТАБНОЙ ОПТИМИЗАЦИИ           |
//|                                                                                 |
//|   Подпрограмма минимизирует  функцию  N  аргументов  F(x)  с  использованием    |
//|   квази-Ньютоновского метода (LBFGS-схема), оптимизированного по использованию  |
//|   оперативной памяти.                                                           |
//|                                                                                 |
//|   Подпрограмма строит аппроксимацию матрицы, обратной к  Гессиану  фунции,  с   |
//|   использованием информации о M предыдущих шагах алгоритма  (вместо N),  что    |
//|   позволяет  снизить  требуемый  объем оперативной памяти с величины порядка    |
//|   N^2 до величины порядка 2*N*M.                                                |
//|                                                                                 |
//|   Данная подпрограмма разработана на основе описания, изложенного  в  статье    |
//|   D.C. Liu и J. Nocedal (1989).                                                 |
//|                                                                                 |
//|   ОБРАТНАЯ КОММУНИКАЦИЯ                                                         |
//|                                                                                 |
//|   В ходе  своей  работы  подпрограмма  использует  обратную  коммуникацию  с    |
//|   пользователем для вычисления значения функции и градиента. Типичная  схема    |
//|   работы выглядит так:                                                          |
//|   1. Вызывается подпрограмма MinLBFGS с указанием параметров задачи (размера,   |
//|      критериев   остановки   и  т.д.).  Состояние  программы  сохраняется  в    |
//|      переданной в качестве одного из параметров структуре State, после  чего    |
//|      управление возвращается пользователю                                       |
//|   2. Если поле State.Stage содержит 0, то задача успешно решена, параметр  X    |
//|      содержит решение. Алгоритм завершен.                                       |
//|   3. В  ином  случае  пользователь  вычисляет  значение функции и градиент в    |
//|      точке State.X (array [0..N-1]),  сохраняет их в  State.F  (вещественное    |
//|      поле)  и State.G (array [0..N-1]),  после  чего  вызывает  подпрограмму    |
//|      MinLBFGSIteration.                                                         |
//|   4. goto 2.                                                                    |
//|                                                                                 |
//|   ПРИМЕР:                                                                       |
//|       N:=3;                                                                     |
//|       M:=2;                                                                     |
//|       x[0]:=0;                                                                  |
//|       x[1]:=0;                                                                  |
//|       x[2]:=0;                                                                  |
//|       MinLBFGS(N, M, X, EpsG, EpsF, EpsX, MaxIts, 0, State);                    |
//|       while State.Stage<>0 do                                                   |
//|       begin                                                                     |
//|           (x-2)^2 + y^2 + (z-x)^2                                               |
//|           State.F:=Sqr(State.X[0]-2)+Sqr(State.X[1])+Sqr(State.X[2]-State.X[0]);|
//|           State.G[0]:= 2*(State.X[0]-2) + 2*(State.X[0]-State.X[2]);            |
//|           State.G[1]:= 2*State.X[1];                                            |
//|           State.G[2]:= 2*(State.X[2]-State.X[0]);                               |
//|           MinLBFGSIteration(X, State, Rep);                                     |
//|       end;                                                                      |
//|                                                                                 |
//|   ВХОДНЫЕ ПАРАМЕТРЫ:                                                            |
//|       N       -   Размерность задачи, N>1                                       |
//|       M       -   Число коррекций в BFGS-схеме обновления аппроксимации         |
//|                   Гессиана. Рекомендуемое   значение:  3 <= M <= 7.  Меньшее    |
//|                   значение не позволит добиться нормальной скорости сходимости, |
//|                   большее - не позволит получить заметный выигрыш в скорости    |
//|                   сходимости, зато приведет к падению быстродействия.           |
//|                   1<=M<=N.                                                      |
//|       X       -   Начальное приближение к решению, array[0..N-1]                |
//|       EpsG    -   Положительное число, определяющее точность поиска минимума    |
//|                   Подпрограмма прекращает  работу, если выполняется  условие    |
//|                   ||G|| < EpsG, где ||.|| обозначает  Евклидову  норму,  G -    |
//|                   градиент, X - текущее приближение к минимуму.                 |
//|       EpsF    -   Положительное число, определяющее точность поиска минимума    |
//|                   Подпрограмма прекращает работу, если на  k+1-ой   итерации    |
//|                   выполняется условие                                           |
//|                   |F(k+1)-F(k)| <= EpsF*max{|F(k)|, |F(k+1)|, 1}                |
//|       EpsX    -   Положительное число, определяющее точность поиска минимума    |
//|                   Подпрограмма прекращает работу, если на  k+1-ой   итерации    |
//|                   выполняется условие |X(k+1)-X(k)| <= EpsX                     |
//|       MaxIts  -   Максимальное число итераций алгоритма.                        |
//|                   Если MaxIts=0, то число итераций не ограничено.               |
//|       Flags   -   дополнительные настройки алгоритма:                           |
//|                   * 0     нет дополнительных настроек                           |
//|                   * 1     не выделять память (используется при решении серии    |
//|                           задач с одними и теми же значениями N и M;  память    |
//|                           выделяется только для первой задачи, при повторных    |
//|                           вызовах можно ускорить быстродействие, отказавшись    |
//|                           от её выделения, если передавать в алгоритм  ранее    |
//|                           инициализированную структуру State). Если структура   |
//|                           State была инициализирована с использованием других   |
//|                           значений N/M, то использование этого  флага  может    |
//|                           привести к сбою в работе подпрограммы.                |
//|                                                                                 |
//|   ВЫХОДНЫЕ ПАРАМЕТРЫ:                                                           |
//|       State   -   структура, сохраняющая состояние алгоритма между повторными   |
//|                   вызовами и использующаяся для обратной  коммуникации.  При    |
//|                   возврате из этой подпрограммы поле State.X содержит точку,    |
//|                   в  которой  требуется  вычислить  значение  функции,  поле    |
//|                   State.G  содержит  инициализированный  массив,  в  который    |
//|                   требуется поместить градиент, а в поле State.F   требуется    |
//|                   поместить значение функции.                                   |
//|       Rep     -   отчет о работе (см. описание подпрограммы MinLBFGSIteration)  |
//|                                                                                 |
//|   СМ. ТАКЖЕ                                                                     |
//|       подпрограмма MinLBFGSIteration                                            |
//|                                                                                 |
//|     -- ALGLIB --                                                                |
//|        Copyright 14.11.2007 by Bochkanov Sergey                                 |
//|                                                                                 |
//+---------------------------------------------------------------------------------+
void MinLBFGS( int N,         // - WCount                - 
               int M,         // - MathMin(WCount, 50)   - 
               double X[],    // - W                     - 
               double EG,     // - MinGVal               - 
               double EF,     // - 0.0                   - 
               double EX,     // - WStep                 - 
               int Max_Its,   // - 0                     - 
               int Flags_,    // - 0                     - 
               int& State,    // - State                 - 
               int& Rep){     // - InternalRep           - 
//--------------------------------------------
//   Print("Процедура MinLBFGS - оптимизация F(x).");
//--------------------------------------------
// Initialize
    LBFGSState[State,LBF_N,0,0] = N;
    LBFGSState[State,LBF_M,0,0] = M;
    LBFGSState[State,EpsG,0,0] = EG;
    LBFGSState[State,EpsF,0,0] = EF;
    LBFGSState[State,EpsX,0,0] = EX;
    LBFGSState[State,MaxIts,0,0] = Max_Its;
    LBFGSState[State,Flags,0,0] = Flags_;
    bool AllocateMem = MathMod(Flags_,2)==0.0;
    Flags_ = MathFloor(Flags_/2);
//--------------------------------------------
// Initialize Rep structure
   LBFGSReport[Rep,TerminationType] = 0.0;
   LBFGSReport[Rep,IterationsCount] = 0.0;
   LBFGSReport[Rep,Rep_NFEV] = 1.0;
   LBFGSState[State,XUpdated,0,0] = 0.0;
//--------------------------------------------
// Подготовитесь сначала управляемый
   LBFGSState[State,LBF_K,0,0] = 0.0;
   for( int i=0; i<N; i++)
      LBFGSState[State,LBF_X,i,0] = X[i];
//--------------------------------------------
// Вычисление F, пойход на следующую стадию
   LBFGSState[State,LBF_Stage,0,0] = 1.0;
//--------------------------------------------
}

//+------------------------------------------------------------------------------+
//|                                                                              |
//|  Вычисление градиента функции ошибки на основе пакета образов, используется  |
//|  естественная функция ошибки                                                 |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      Network -   структура, задающая обученную/необученную нейронную  сеть.  |
//|                  структура передается по ссылке  и  модифицируется  в  ходе  |
//|                  работы (т.к. её поля используются для  хранения  временных  |
//|                  переменных).                                                |
//|      XY      -   пакет образов.                                              |
//|                  массив размером [0..SSize-1,0..NIn+NOut-1].                 |
//|                  первые   NIn   столбцов   содержат   входные   данные,      |
//|                  последующие NOut столбцов  содержат желаемые  значения      |
//|                  выходов сети.                                               |
//|      SSize   -   размер пакета                                               |
//|                                                                              |
//|  Выходные параметры:                                                         |
//|      E       -   функция ошибки                                              |
//|      Grad    -   градиент функции ошибки по весовым/пороговым коэффициентам. |
//|                  Подпрограмма  не  выделяет  память  под  этот  вектор, это  |
//|                  обязанность   того,  кто  вызывает  подпрограмму, выделить  |
//|                  память под результат.                                       |
//|                  Массив должен иметь  размер [0..WCount-1].                  |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//|                                                                              |
//+------------------------------------------------------------------------------+
void MLPGradNBatch( int Network,       // структура НС                     - (входной)
                     double& XY[],     // Исходный массив данных (опытов)  - (входной)
                     int SSize,        // Размер массива (кол-во опытов)   - (входной)
                     double& E,        // Функция ошибки                   - (выходной)
                     double& Grad[]){  // градиент ф-ии ошибки             - (выходной)

//--------------------------------------------
//   Print("Процедура вычисления градиента - 1.");
//--------------------------------------------
   int i, NIn, NOut, WCount;
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);   // по заданой структуре устанавливает значения - NIn, NOut, WCount
//--------------------------------------------
   ArrayResize(Grad,WCount);
   ArrayInitialize(Grad,0.0);
//--------------------------------------------
   E = 0.0;
   i = 0.0;
   while( i<SSize ){
      MLPChunkedGradient(Network, XY, i, MathMin(SSize, i+ChunkSize)-i, E, Grad, True);
      i += ChunkSize;
   }
//--------------------------------------------
}

//+---------------------------------------------------------------------------------+
//|   Вычисление градиента функции ошибки на основе пакета образов                  |
//|                                                                                 |
//|   Входные параметры:                                                            |
//|       Network -   структура, задающая обученную/необученную нейронную  сеть.    |
//|                   структура передается по ссылке  и  модифицируется  в  ходе    |
//|                   работы (т.к. её поля используются для  хранения  временных    |
//|                   переменных).                                                  |
//|       XY      -   пакет образов.                                                |
//|                   массив размером [0..SSize-1,0..NIn+NOut-1].                   |
//|                   первые   NIn   столбцов   содержат   входные   данные,        |
//|                   последующие NOut столбцов  содержат желаемые  значения        |
//|                   выходов сети.                                                 |
//|       SSize   -   размер пакета                                                 |
//|                                                                                 |
//|   Выходные параметры:                                                           |
//|       E       -   функция ошибки, SUM(sqr(y[i]-desy[i])/2,i)                    |
//|       Grad    -   градиент функции ошибки по весовым/пороговым коэффициентам.   |
//|                   Подпрограмма  не  выделяет  память  под  этот  вектор, это    |
//|                   обязанность   того,  кто  вызывает  подпрограмму, выделить    |
//|                   память под результат.                                         |
//|                   Массив должен иметь  размер [0..WCount-1].                    |
//|                                                                                 |
//|     -- ALGLIB --                                                                |
//|        Copyright 04.11.2007 by Bochkanov Sergey                                 |
//+---------------------------------------------------------------------------------+

void MLPGradBatch( int Network,        // структура НС                     - (входной)
                     double& XY[][],     // Исходный массив данных (опытов)  - (входной)
                     int SSize,        // Размер массива (кол-во опытов)   - (входной)
                     double& E,        // Функция ошибки                   - (выходной)
                     double& Grad[]){  // градиент ф-ии ошибки             - (выходной)
//--------------------------------------------
//   Print("Процедура вычисления градиента - 1.");
//--------------------------------------------
   int i, NIn, NOut, WCount;
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);   // по заданой структуре (Network) устанавливает значения - NIn, NOut, WCount
//--------------------------------------------
   ArrayResize(Grad,WCount);
   ArrayInitialize(Grad,0.0);
//--------------------------------------------
    E = 0.0;
    i = 0.0;
    while( i<SSize ){
      MLPChunkedGradient(Network, XY, i, MathMin(SSize, i+ChunkSize)-i, E, Grad, False);
      i = i+ChunkSize;
    }
//--------------------------------------------
}


//+--------------------------------------------------------------------------------+
//+--------------------------------------------------------------------------------+
//|                                                                                |
//| Внутренняя подпрограмма, chunked градиент                                      |
//|                                                                                |
//+--------------------------------------------------------------------------------+
//+--------------------------------------------------------------------------------+
void MLPChunkedGradient(int Network, 
                        double& XY[][], 
                        int CStart, 
                        int CSize, 
                        double& E, 
                        double& Grad[], 
                        bool NaturalErrorFunc){
//--------------------------------------------
   int N1, N2, W1, W2, C1, C2, NTotal, NIn, NOut, Offs,
       IStart, INeurons, IDFDNET, IDError, IZeros, i1_;
   double dEdF, dFdNET, F, DF, D2F, V, S, FOwn, DEOwn, NET, LnNET, MX;
   bool BFlag;
//--------------------------------------------
//   Print("Процедура вычисления градиента - 2.");
//--------------------------------------------
// Чтение геометрии сети, подготовкае данных
   NIn = MultiLayerPerceptron[Network,StructInfo,1,0];
   NOut = MultiLayerPerceptron[Network,StructInfo,2,0];
   NTotal = MultiLayerPerceptron[Network,StructInfo,3,0];
   IStart = MultiLayerPerceptron[Network,StructInfo,5,0];
   C1 = CStart;
   C2 = CStart + CSize - 1.0;
   INeurons = 0.0;
   IDFDNET = NTotal;
   IDError = 2*NTotal;
   IZeros = 3*NTotal;
   for( int j=0; j<CSize; j++)
      MultiLayerPerceptron[Network,Chunks,IZeros,j] = 0.0;
/*--------------------------------------------------------------
  Отправьте проход: 
  1. Груз вводит от XY до Кусков [0:NIn-1,0:CSize-1] 
  2. Отправьте проход 
//-----------------------------------------------------------*/
   for( int i=0; i<NIn; i++)
      for( j=0; j<CSize; j++)
         if( MultiLayerPerceptron[Network,ColumnSigmas,i,0]!=0 )
            MultiLayerPerceptron[Network,Chunks,i,j] = 
               (XY[(C1+j),i] - MultiLayerPerceptron[Network,ColumnMeans,i,0]) /
                  MultiLayerPerceptron[Network,ColumnSigmas,i,0];
         else
            MultiLayerPerceptron[Network,Chunks,i,j] = 
               XY[C1+j,i] - MultiLayerPerceptron[Network,ColumnMeans,i,0];
//--------------------------------------------
   for( i=0; i<NTotal; i++){
      Offs = IStart + i*NFieldWidth;
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]>0 ) {
//--------------------------------------------
// Функция активации:
// * вычисление вектора F, F(i) = F(NET(i))
         N1 = MultiLayerPerceptron[Network,StructInfo,(Offs+2),0];
         for( j=0; j<CSize; j++)
            MultiLayerPerceptron[Network,Chunks,i,j] = MultiLayerPerceptron[Network,Chunks,N1,j];
//--------------------------------------------
         for( j=0; j<CSize; j++){

//------ ВХОД В ОШИБКУ - ДЕЛЕНИЕ НА НОЛЬ ---------------------------------------------------------------
// MLPActivationFunction(double NET, int K, double& F, double& DF, double& D2F )
// NET   = MultiLayerPerceptron[Network,Chunks,i,j]
// K     = MultiLayerPerceptron[Network,StructInfo,Offs,0]
// F     = F
// DF    = DF
// D2F   = D2F
            MLPActivationFunction(MultiLayerPerceptron[Network,Chunks,i,j],
                                    MultiLayerPerceptron[Network,StructInfo,Offs,0], F, DF, D2F);
            MultiLayerPerceptron[Network,Chunks,i,j] = F;
            MultiLayerPerceptron[Network,Chunks,(IDFDNET+i),j] = DF;
         } // for J
//--------------------------------------------
      } // if
//--------------------------------------------
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==0 ) {
// Функция активации:
// * вычисление вектора NET, NET(i) = SUM(W(j,i)*Neurons(j),j=N1..N2)
         N1 = MultiLayerPerceptron[Network,StructInfo,(Offs+2),0];
         N2 = N1+MultiLayerPerceptron[Network,StructInfo,(Offs+1),0]-1.0;
         W1 = MultiLayerPerceptron[Network,StructInfo,(Offs+3),0];
         W2 = W1+MultiLayerPerceptron[Network,StructInfo,(Offs+1),0]-1.0;
//--------------------------------------------
         for( j= 0; j<CSize; j++)
            MultiLayerPerceptron[Network,Chunks,i,j] = MultiLayerPerceptron[Network,Chunks,IZeros,j];
//--------------------------------------------
         for( j=N1; j<=N2; j++){
            V = MultiLayerPerceptron[Network,Weights,(W1+j-N1),0];
            for( int k=0; k<CSize; k++)
               MultiLayerPerceptron[Network,Chunks,i,k] += V*MultiLayerPerceptron[Network,Chunks,j,k];
         } // for J
      } // if
//--------------------------------------------
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]<0 ) {
         BFlag = False;
         if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==-2 ) 
            BFlag = True;  // нейрон входа, оставлен неизменным
//--------------------------------------------
         if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==-3 ) {
            for( k=0; k<CSize; k++) 
               MultiLayerPerceptron[Network,Chunks,i,k] = -1.0;
            BFlag = True;                   // "-1" neuron
         } // if
//--------------------------------------------
         if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==-4 ) {
            for( k=0; k<CSize; k++)
               MultiLayerPerceptron[Network,Chunks,i,k] = 0.0;
            BFlag = True;    // "0" neuron
         } // if
//--------------------------------------------
      } // if
//--------------------------------------------
   } // for I
//--------------------------------------------
// Постобработка, ошибка, dError/dOut
   for( i=0; i<NTotal; i++)
      for( j=0; j<CSize; j++)
         MultiLayerPerceptron[Network,Chunks,(IDError+i),j] = MultiLayerPerceptron[Network,Chunks,IZeros,j];
   
/********************************************************************************
        Продукция Мax, сеть классификации. 
        Для каждого K = 0.. CSize-1 делают: 
        1. поместить exp (продукции [k]) к NWBuf [0:NOut-1] 
        2. поместите сумму (exp (..)) к ЧИСТЫМ 
        3. вычислите dError/dOut и поместите это во второй блок Кусков
*********************************************************************************/
   if( MultiLayerPerceptron[Network,StructInfo,6,0]==1 ) {
      for( k=0; k<CSize; k++){
//--------------------------------------------
// Нормализация
         MX = MultiLayerPerceptron[Network,Chunks,NTotal-NOut,k];
//--------------------------------------------
         for( i=1; i<NOut; i++)
            MX = MathMax(MX, MultiLayerPerceptron[Network,Chunks,(NTotal-NOut+i),k]);
         NET = 0.0;
//--------------------------------------------
         for( i=0; i<NOut; i++){
            MultiLayerPerceptron[Network,NWBuf,i,0] = MathExp(MultiLayerPerceptron[Network,Chunks,(NTotal-NOut+i),k]-MX);
            NET += MultiLayerPerceptron[Network,NWBuf,i,0];
         } // for I
//--------------------------------------------
// Вычисление ошибки функции и dError/dOut
         if( NaturalErrorFunc ) {
//--------------------------------------------
// Нормализация ошибки функции
            S = 0.0;
            for( i=0; i<NOut; i++)
               S += XY[(CStart+k),(NIn+i)];
//--------------------------------------------
            LnNET = MathLog(NET);
//--------------------------------------------
            for( i=0; i<NOut; i++){
               MultiLayerPerceptron[Network,Chunks,(IDError+NTotal-NOut+i),k] = 
                                S*MultiLayerPerceptron[Network,NWBuf,i,0]/NET-XY[CStart+k,NIn+i];
               E += SafeCrossEntropy((XY[(CStart+k),(NIn+i)]), (MultiLayerPerceptron[Network,NWBuf,i,0]/NET));
            } // for I
//--------------------------------------------
         } else {
//--------------------------------------------
// Ошибка наименьших квадратов func
// Ошибка, dError/dOut(нормализованых)
            for( i=0; i<NOut; i++){
               V = MultiLayerPerceptron[Network,NWBuf,i,0]/NET-XY[(CStart+k),(NIn+i)];
               MultiLayerPerceptron[Network,NWBuf,NOut+i,0] = V;
               E += MathPow(V,2)/2.0;
            } // for I
//--------------------------------------------
// От dError/dOut (нормализованного) к (ненормализованному) dError/dOut
            i1_ = 0 - NOut;
            V = 0.0;
            for( int i_=NOut; i_<2*NOut; i_++)
               V += MultiLayerPerceptron[Network,NWBuf,i_,0]*MultiLayerPerceptron[Network,NWBuf,(i_+i1_),0];
//--------------------------------------------
            for( i=0; i<NOut; i++){
               FOwn = MultiLayerPerceptron[Network,NWBuf,i,0];
               DEOwn = MultiLayerPerceptron[Network,NWBuf,(NOut+i),0];
               MultiLayerPerceptron[Network,Chunks,(IDError+NTotal-NOut+i),k] = 
                                 (-V+DEOwn*FOwn+DEOwn*(NET-FOwn))*FOwn/MathPow(NET,2);
            } // for I
//--------------------------------------------
         } // if
//--------------------------------------------
      } // for K
//--------------------------------------------
   } else {
   
/********************************************************************************
         Нормальная продукция, сеть регресса 
         
         Для каждого K = 0.. CSize-1 делают: 
         1. вычислить dError/dOut и поместить это во второй блок Кусков 
*********************************************************************************/
      for( i=0; i<NOut; i++)
         for( j=0; j<CSize; j++){
            V = MultiLayerPerceptron[Network,Chunks,(NTotal-NOut+i),j]*
                  MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0]+
                     MultiLayerPerceptron[Network,ColumnMeans,(NIn+i),0] - XY[CStart+j,NIn+i];
            MultiLayerPerceptron[Network,Chunks,(IDError+NTotal-NOut+i),j] = 
                              V*MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0];
            E += MathPow(V,2)/2.0;
         } // for J
//--------------------------------------------
   } // if
//--------------------------------------------
// Обратная связь
   for( i=NTotal-1; i>=0; i--){
// Извлечение информации
      Offs = IStart + i*NFieldWidth;
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]>0 ) {
// Активация функции
         N1 = MultiLayerPerceptron[Network,StructInfo,(Offs+2),0];
//--------------------------------------------
         for( k=0; k<CSize; k++)
            MultiLayerPerceptron[Network,Chunks,(IDError+i),k] *= 
                  MultiLayerPerceptron[Network,Chunks,(IDFDNET+i),k];
//--------------------------------------------
         for( i_=0; i_<CSize; i_++)
                MultiLayerPerceptron[Network,Chunks,(IDError+N1),i_] += 
                     MultiLayerPerceptron[Network,Chunks,(IDError+i),i_];
//--------------------------------------------
      } // if
//--------------------------------------------
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==0 ) {
// "Normal" activation function
         N1 = MultiLayerPerceptron[Network,StructInfo,(Offs+2),0];
         N2 = N1+MultiLayerPerceptron[Network,StructInfo,(Offs+1),0]-1.0;
         W1 = MultiLayerPerceptron[Network,StructInfo,(Offs+3),0];
         W2 = W1+MultiLayerPerceptron[Network,StructInfo,(Offs+1),0]-1.0;
//--------------------------------------------
         for( j=W1; j<=W2; j++){
            V = 0.0;
            for( i_=0; i_<CSize; i_++)
               V += MultiLayerPerceptron[Network,Chunks,(N1+j-W1),i_]*
                        MultiLayerPerceptron[Network,Chunks,(IDError+i),i_];
            Grad[j] = Grad[j]+V;
         }  // for J
//--------------------------------------------
         for( j=N1; j<=N2; j++){
            V = MultiLayerPerceptron[Network,Weights,(W1+j-N1),0];
            for( i_=0; i_<CSize; i_++)
               MultiLayerPerceptron[Network,Chunks,(IDError+j),i_] += 
                     V*MultiLayerPerceptron[Network,Chunks,(IDError+i),i_];
         }  // for J
//--------------------------------------------
      } // if
//--------------------------------------------
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]<0 ) {
         BFlag = False;
         if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==-2 || 
               MultiLayerPerceptron[Network,StructInfo,Offs,0]==-3 || 
                  MultiLayerPerceptron[Network,StructInfo,Offs,0]==-4 ) 
            BFlag = True;                                                  // Special neuron type, no back-propagation required
      } // if
//--------------------------------------------
   } // for I
//--------------------------------------------
}

//+--------------------------------------------------------------------------------+
//|  Функция активации нейронной сети                                              |
//|                                                                                |
//| Входные параметры:                                                             |
//|    NET -   вход нейрона                                                        |
//|    K   -   тип функции активации                                               |
//|                                                                                |
//| Выходные параметры:                                                            |
//|    F   -   значение функции F(NET)                                             |
//|    DF  -   производная функции по NET.                                         |
//|    D2F -   вторая производная функции по NET                                   |
//|                                                                                |
//|  -- ALGLIB --                                                                  |
//|     Copyright 04.11.2007 by Bochkanov Sergey                                   |
//+--------------------------------------------------------------------------------+
void MLPActivationFunction(double NET, 
                           int K, 
                           double& F, 
                           double& DF, 
                           double& D2F ){
//--------------------------------------------
   double NET2, ARG, ROOT, R, F1;
//--------------------------------------------
//   Print("Процедура активации НС - MLPActivationFunction.");
//--------------------------------------------
   F = 0.0;
   F1 = 0.0;
   DF = 0.0;
   if( K==1 ) {
// Функция активации TanH

//--------- ОШИБКА ДЕЛЕНИЯ НА НОЛЬ В ФУНКЦИИ TanH(NET) КОГДА NET ПРЕВЫШАЕТ ГРАНИЦЫ ВОЗМОЖНОЙ ЦИФРЫ --------------------

      F = TanH(NET);
      DF = 1-MathPow(F,2);
      D2F = -(2*F*DF);
      return(0);        // Exit Sub
   } // if
//--------------------------------------------
   if( K==3 ) {
// EX (ИСКЛЮЧАЯ) функцией активации
      if( NET>=0 ) {
         NET2 = MathPow(NET,2);
         ARG = NET2+1.0;
         ROOT = MathSqrt(ARG);
         F = NET+ROOT;
         R = NET/ROOT;
         DF = 1+R;
         D2F = (ROOT-NET*R)/ARG;
      } else {
         F = MathExp(NET);
         DF = F;
         D2F = F;
      }
      return(0);     // Exit Sub
   }
//--------------------------------------------
   if( K==2 ) {
      F = MathExp(-MathPow(NET,2));
      DF = -(2*NET*F);
      D2F = -(2*(F+DF*NET));
      return(0);     // Exit Sub
    }
}

//+------------------------------------------------------------------------------+
//|                                                                              |
//| Возвращения T*Ln (T/Z), предотвращающий выход за пределы потока              |
//| Внутреннzz подпрограммы.                                                     |
//|                                                                              |
//+------------------------------------------------------------------------------+
double SafeCrossEntropy( double T, double Z ){
//--------------------------------------------
   double Result, R;
//--------------------------------------------
//   Print("Процедура проверки порогов.");
//--------------------------------------------
   if( T==0 )
      return(0);
   else {
      if( MathAbs(Z)>1 )
// Shouldn//t be the case with softmax,
// but we just want to be sure.
         if( T/Z==0 )
            R = MathPow(0.1,300);     // MinRealNumber;
         else
            R = MathAbs(T/Z);
      else 
// В нормальном случае
         if( (Z==0) || (MathAbs(T)>=MathPow(10.0,300)*MathAbs(Z)) )  // MaxRealNumber*MathAbs(Z) )
            R = MathPow(10.0,300);                                   // MaxRealNumber;
         else
            R = MathAbs(T/Z);
       Result = T*MathLog(R);          // MathLog(R);
   }
   return(Result);
}

//+------------------------------------------------------------------------------+
//|  Одна итерация L-BFGS алгоритма.                                             |
//|                                                                              |
//|  Вызывается после инициализации алгоритма подпрограммой MinLBFGS. Пример     |
//|  использования приведен в описании подпрограммы MinLBFGS.                    |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      State   -   структура,   сохраняющая   состояние    алгоритма    между  |
//|                  повторными  вызовами   и   использующаяся   для   обратной  |
//|                  коммуникации. Эта структура должна  быть  инициализирована  |
//|                  подпрограммой MinLBFGS.                                     |
//|                  Поле State.F должно содержать  значение  функции  в  точке  |
//|                  State.X, а State.G - градиент в той же точке.               |
//|      Rep     -   отчет о текущем состоянии алгоритма оптимизации,            |
//|                  обновляется на каждой итерации.                             |
//|                                                                              |
//|  Выходные параметры:                                                         |
//|      X       -   если State.Stage равно 0, то содержит решение задачи.       |
//|      State   -   Если State.Stage равно 0, то алгоритм завершил  работу.  В  |
//|                  ином  случае  State.X (array [0..N-1]) содержит  точку,  в  |
//|                  которой требуется вычислить  значение функции и градиент,   |
//|                  а затем повторно вызвать подпрограмму.                      |
//|      Rep     -   если State.Stage не равно 0, то содержит отчет о работе:    |
//|                  * Rep.TerminationType содержит причину завершения алгоритма:|
//|                      * -2    ошибки округления препятствуют дальнейшей       |
//|                              работе алгоритма. X содержит наилучшую точку    |
//|                              из найденных.                                   |
//|                      * -1    указаны неверные параметры                      |
//|                      *  1    относительное уменьшение функции не превосходит |
//|                              EpsF.                                           |
//|                      *  2    изменение текущего приближения не превосходит   |
//|                              EpsX.                                           |
//|                      *  4    норма градиента не превосходит EpsG             |
//|                      *  5    превышено максимальное число итераций MaxIts    |
//|                  * Rep.IterationsCount  содержит  число  итераций алгоритма  |
//|                    (изменяется  в   начале   каждой   итерации,  что  может  |
//|                    служить признаком её начала).                             |
//|                  * NFEV содержит число выходов из алгоритма для  вычисления  |
//|                    значения функции.                                         |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 14.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+
void MinLBFGSIteration( double& X[],
                        int& State, 
                        int& Rep ){
//--------------------------------------------
   int Ind, N, M, Max_Its, IC, MCINFO, NFEVm, Stg;
   double EF, EG, EX, V, VV, Fm, STPm, Xm[], Gm[], Sm[], Wm[];
//--------------------------------------------
//   Print("Процедура L-BFGS алгоритма.");
//--------------------------------------------
// Разгрузкае часто используемые переменные от государственной структуры (только для того, чтобы печатать convinience)
   N = LBFGSState[State,LBF_N,0,0];
   M = LBFGSState[State,LBF_M,0,0];
   EG = LBFGSState[State,EpsG,0,0];
   EF = LBFGSState[State,EpsF,0,0];
   EX = LBFGSState[State,EpsX,0,0];
   Max_Its = LBFGSState[State,MaxIts,0,0];
//--------------------------------------------
   ArrayResize(Xm,N);
   ArrayResize(Gm,N);
   ArrayResize(Sm,N);
   ArrayResize(Wm,N);
//--------------------------------------------
// Обновление информации
   LBFGSState[State,XUpdated,0,0] = 0.0;
//--------------------------------------------
// Переход в стадии
   while (True){
// Стадия 1: подготовитесь к первому пробегу
      if( LBFGSState[State,LBF_Stage,0,0]==1 ) {
//--------------------------------------------
// Подготовка конца к первому пробегу
         LBFGSState[State,FOld,0,0] = LBFGSState[State,LBF_F,0,0];
         V = 0.0;
         for( int i=0; i<N; i++)
            V += MathPow(LBFGSState[State,LBF_G,i,0],2);
         V = MathSqrt(V);
//--------------------------------------------
         if( V==0 ) {
            LBFGSReport[Rep,TerminationType] = 4.0;
            LBFGSState[State,LBF_Stage,0,0] = 0.0;
            return(0);
         } // if
//--------------------------------------------
         LBFGSState[State,Stp,0,0] = 1.0/V;
         for( i=0; i<N; i++)
            LBFGSState[State,LBF_D,i,0] = -LBFGSState[State,LBF_G,i,0];
//--------------------------------------------
// Главный цикл Goto (реальный продолжаются, не выход), 
         LBFGSState[State,LBF_Stage,0,0] = 2.0;
         continue; // GoTo Cont_1
      }
//--------------------------------------------
// Стадия 2: главный цикл, фаза 1
      if( LBFGSState[State,LBF_Stage,0,0]==2 ) {
//--------------------------------------------
// Главный цикл: подготовитесь к 1-d поиску линии
         LBFGSState[State,LBF_P,0,0] = MathMod(LBFGSState[State,LBF_K,0,0], M);
         LBFGSState[State,LBF_Q,0,0] = MathMin(LBFGSState[State,LBF_K,0,0], M-1);
         Ind = LBFGSState[State,LBF_P,0,0];
//--------------------------------------------
// Склад X[k], G[k]
         for( i=0; i<N; i++)
            LBFGSState[State,LBF_S,Ind,i] = -LBFGSState[State,LBF_X,i,0]; // State.S(State.P,i_) = -State.X(i_)
//--------------------------------------------
         for( i=0; i<N; i++)
            LBFGSState[State,LBF_Y,Ind,i] = -LBFGSState[State,LBF_G,i,0];
//--------------------------------------------
// Минимизация F(x+alpha*d)
         LBFGSState[State,MCStage,0,0] = 0.0;
//--------------------------------------------
         if( LBFGSState[State,LBF_K,0,0]!=0 )
            LBFGSState[State,Stp,0,0] = 1.0;
//--------------------------------------------
         for( i=0; i<N; i++){
            Xm[i] = LBFGSState[State,LBF_X,i,0];         // double& X[]
            Gm[i] = LBFGSState[State,LBF_G,i,0];         // double& G[]
            Sm[i] = LBFGSState[State,LBF_D,i,0];         // double& S[]
            Wm[i] = LBFGSState[State,WORK,i,0];          // double& WA[]
         }
            Fm = LBFGSState[State,LBF_F,0,0];         // double F
            STPm = LBFGSState[State,Stp,0,0];         // double STP
            NFEVm = LBFGSState[State,LBF_NFEV,0,0];   // int NFEV
            Stg = LBFGSState[State,MCStage,0,0];      // int Stage
//--------------------------------------------
         MCSRCH(N, Xm, Fm, Gm, Sm, STPm, MCINFO, NFEVm, Wm, State, Stg);
//--------------------------------------------
         for( i=0; i<N; i++){
            LBFGSState[State,LBF_X,i,0] = Xm[i];         // double& X[]
            LBFGSState[State,LBF_G,i,0] = Gm[i];         // double& G[]
            LBFGSState[State,LBF_D,i,0] = Sm[i];         // double& S[]
            LBFGSState[State,WORK,i,0] = Wm[i];          // double& WA[]
         }
            LBFGSState[State,LBF_F,0,0] = Fm;         // double F
            LBFGSState[State,Stp,0,0] = STPm;         // double STP
            LBFGSState[State,LBF_NFEV,0,0] = NFEVm;   // int NFEV
            LBFGSState[State,MCStage,0,0] = Stg;      // int Stage
//--------------------------------------------
         LBFGSState[State,LBF_Stage,0,0] = 3.0;
//--------------------------------------------
         if( LBFGSState[State,MCStage,0,0]==0 )
            LBFGSState[State,LBF_Stage,0,0] = 4.0;
         return(0); // Exit Sub
      }  // if
//--------------------------------------------
// Главный цикл: поиск линии 1-d
      if( LBFGSState[State,LBF_Stage,0,0]==3 ) {
//--------------------------------------------
         for( i=0; i<N; i++){
            Xm[i] = LBFGSState[State,LBF_X,i,0];         // double& X[]
            Gm[i] = LBFGSState[State,LBF_G,i,0];         // double& G[]
            Sm[i] = LBFGSState[State,LBF_D,i,0];         // double& S[]
            Wm[i] = LBFGSState[State,WORK,i,0];          // double& WA[]
         }
            Fm = LBFGSState[State,LBF_F,0,0];         // double F
            STPm = LBFGSState[State,Stp,0,0];         // double STP
            NFEVm = LBFGSState[State,LBF_NFEV,0,0];   // int NFEV
            Stg = LBFGSState[State,MCStage,0,0];      // int Stage
//--------------------------------------------
         MCSRCH(  N, Xm, Fm, Gm, Sm, STPm, MCINFO, NFEVm, Wm, State, Stg);
//--------------------------------------------
         for( i=0; i<N; i++){
            LBFGSState[State,LBF_X,i,0] = Xm[i];         // double& X[]
            LBFGSState[State,LBF_G,i,0] = Gm[i];         // double& G[]
            LBFGSState[State,LBF_D,i,0] = Sm[i];         // double& S[]
            LBFGSState[State,WORK,i,0] = Wm[i];          // double& WA[]
         }
            LBFGSState[State,LBF_F,0,0] = Fm;         // double F
            LBFGSState[State,Stp,0,0] = STPm;         // double STP
            LBFGSState[State,LBF_NFEV,0,0] = NFEVm;   // int NFEV
            LBFGSState[State,MCStage,0,0] = Stg;      // int Stage
//--------------------------------------------
         if( LBFGSState[State,MCStage,0,0]==0 ) {
            LBFGSState[State,LBF_Stage,0,0] = 4.0;
            continue;   // GoTo Cont_1
         }
//--------------------------------------------
         return(0);  // Exit Sub
      }
//--------------------------------------------
// Главный цикл: обновите информацию и Мешковину. 
// Проверьте останавливающиеся условия.
      if( LBFGSState[State,LBF_Stage,0,0]==4 ) {
         LBFGSReport[Rep,Rep_NFEV] += LBFGSState[State,LBF_NFEV,0,0];
         LBFGSReport[Rep,IterationsCount] += 1.0;
//--------------------------------------------
// Обновление X
         for( i=0; i<N; i++)
            X[i] = (LBFGSState[State,LBF_X,i,0]);
//--------------------------------------------
// Вычисление S[k], Y[k], Rho[k], GammaK
         for( i=0; i<N; i++)
            LBFGSState[State,LBF_S,Ind,i] += LBFGSState[State,LBF_X,i,0];
//--------------------------------------------
         for( i=0; i<N; i++)
            LBFGSState[State,LBF_Y,Ind,i] += LBFGSState[State,LBF_G,i,0];
//--------------------------------------------
// Условия остановки
         if( ((LBFGSState[State,LBF_K,0,0]+1)>Max_Its) && (Max_Its>0) ) {
// Слишком много повторений
            LBFGSReport[Rep,TerminationType] = 5.0;
            LBFGSState[State,LBF_Stage,0,0] = 0.0;
            return(0);     // Exit Sub
         } // if
//--------------------------------------------
         V = 0.0;
         for( i=0; i<N; i++)
            V += MathPow(LBFGSState[State,LBF_G,i,0],2);
//--------------------------------------------
         if( MathSqrt(V)<=EG ) {
// Градиент является достаточно маленьким
            LBFGSReport[Rep,TerminationType] = 4.0;
            LBFGSState[State,LBF_Stage,0,0] = 0.0;
            return(0);     // Exit Sub
         } // if
//--------------------------------------------
         if( (LBFGSState[State,FOld,0,0]-LBFGSState[State,LBF_F,0,0])<=
                  EF*MathMax(MathAbs(LBFGSState[State,FOld,0,0]), MathMax(MathAbs(LBFGSState[State,FOld,0,0]), 1.0)) ) {
// F(k+1)-F(k) is small enough
            LBFGSReport[Rep,TerminationType] = 1.0;
            LBFGSState[State,LBF_Stage,0,0] = 0.0;
            return(0);     // Exit Sub
         } // if
//--------------------------------------------
         V = 0.0;
         for( i=0; i<N; i++)
            V += MathPow(LBFGSState[State,LBF_S,Ind,i],2);
//--------------------------------------------
         if( MathSqrt(V)<=EX ) {
// X(k+1)-X(k) является достаточно маленьким
            LBFGSReport[Rep,TerminationType] = 2.0;
            LBFGSState[State,LBF_Stage,0,0] = 0.0;
            return(0);     // Exit Sub
         } // if
//--------------------------------------------
// Вычисление Rho[k], GammaK
         V = 0.0;
         for( i=0; i<N; i++)
            V += LBFGSState[State,LBF_Y,Ind,i]*LBFGSState[State,LBF_S,Ind,i];
//--------------------------------------------
         VV = 0.0;
         for( i=0; i<N; i++)
            VV += MathPow(LBFGSState[State,LBF_Y,Ind,i],2);
//--------------------------------------------
         if( V==0 || VV==0 ) {
// Округление ошибок делает дальнейшие повторения невозможными.
            LBFGSReport[Rep,TerminationType] = -2.0;
            LBFGSState[State,LBF_Stage,0,0] = 0.0;
            return(0);     // Exit Sub
         } // if
//--------------------------------------------
         LBFGSState[State,Rho,Ind,0] = 1/V;
         LBFGSState[State,GammaK,0,0] = V/VV;
//+------------------------------------------------------------------+
//  Вычисление d(k+1) = H(k+1)*g(k+1)
//                         
//  for I:=K downto K-Q do
//      V = s(i)^T * work(iteration:I)
//      theta(i) = V
//      work(iteration:I+1) = work(iteration:I) - V*Rho(i)*y(i)
//  work(last iteration) = H0*work(last iteration)
//  for I:=K-Q to K do
//      V = y(i)^T*work(iteration:I)
//      work(iteration:I+1) = work(iteration:I) +(-V+theta(i))*Rho(i)*s(i)
//                           
//  NOW WORK CONTAINS d(k+1)
//+------------------------------------------------------------------+
         for( i=0; i<N; i++)
            LBFGSState[State,WORK,i,0] = LBFGSState[State,LBF_G,i,0];
//--------------------------------------------
         for( i=(LBFGSState[State,LBF_K,0,0]); i>=(LBFGSState[State,LBF_K,0,0]-LBFGSState[State,LBF_Q,0,0]); i--){
            IC = MathMod(i,M);
//--------------------------------------------
            V = 0.0;
            for( int j=0; j<N; j++)
               V += LBFGSState[State,LBF_S,IC,j]*LBFGSState[State,WORK,j,0];
//--------------------------------------------
            LBFGSState[State,Theta,IC,0] = V;
            VV = V*LBFGSState[State,Rho,IC,0];
            for( j=0; j<N; j++)
               LBFGSState[State,WORK,j,0] -= VV*LBFGSState[State,LBF_Y,IC,j];
//--------------------------------------------
         } // for I
//--------------------------------------------
         V = LBFGSState[State,GammaK,0,0];
         for( i=0; i<N; i++)
            LBFGSState[State,WORK,i,0] = V*LBFGSState[State,WORK,i,0];
//--------------------------------------------
         for( i=(LBFGSState[State,LBF_K,0,0]-LBFGSState[State,LBF_Q,0,0]); i<=(LBFGSState[State,LBF_K,0,0]); i++){
            IC = MathMod(i,M);
            V = 0.0;
//--------------------------------------------
            for( j=0; j<N; j++)
               V += LBFGSState[State,LBF_Y,IC,j]*LBFGSState[State,WORK,j,0];
//--------------------------------------------
            VV = LBFGSState[State,Rho,IC,0]*(-V+LBFGSState[State,Theta,IC,0]);
            for( j=0; j<N; j++)
               LBFGSState[State,WORK,j,0] += VV*LBFGSState[State,LBF_S,IC,j];
//--------------------------------------------
         } // for I
//--------------------------------------------
         for( i=0; i<N; i++)
            LBFGSState[State,LBF_D,i,0] = -LBFGSState[State,WORK,i,0];
//--------------------------------------------
// Следующий шаг
         LBFGSState[State,FOld,0,0] = LBFGSState[State,LBF_F,0,0];
         LBFGSState[State,LBF_K,0,0] = LBFGSState[State,LBF_K,0,0]+1.0;
         LBFGSState[State,XUpdated,0,0] = 1.0;
         LBFGSState[State,LBF_Stage,0,0] = 2.0;
         continue; // GoTo Cont_1
      }
//--------------------------------------------
   }  // Loop
//--------------------------------------------
}

//+------------------------------------------------------------------------------------+
//|   Цель mcsrch состоит в том, чтобы найти шаг, который удовлетворяет                |
//|   достаточное условие Уменьшения и условие искривления.                            |
//|                                                                                    |
//|   В каждой стадии подпрограмма обновляет интервал неуверенности с                  |
//|   Конечными точками stx и stp. Интервал неуверенности первоначально                |
//|   выбран Так, чтобы это содержало минимизирование измененной функции               |
//|                                                                                    |
//|      F(X+STP*S) - F(X) - FTOL*STP*(GRADF(X)/S).                                    |
//|                                                                                    |
//|   Алгоритм разработан, чтобы найти шаг, который удовлетворяет достаточное          |
//|   условие Уменьшения                                                               |
//|                                                                                    |
//|      F(X+STP*S) .LE. F(X) + FTOL*STP*(GRADF(X)/S),                                 |
//|                                                                                    |
//|   И условие искривления                                                            |
//|                                                                                    |
//|      ABS(GRADF(X+STP*S)/S)) .LE. GTOL*ABS(GRADF(X)/S).                             |
//|                                                                                    |
//|   Если ftol - меньше чем gtol и если, например, функция ограничена Ниже,           |
//|   то есть всегда шаг, который удовлетворяет оба условия. Если никакой шаг          |
//|   не может быть найден, который удовлетворяет оба условия, то Алгоритм обычно      |
//|   останавливается, когда округляющиеся ошибки предотвращают далее продвижение.     |
//|   В этом случае НТП только удовлетворяет достаточное условие уменьшения.           |
//|                                                                                    |
//|   Параметры дисперии                                                               |
//|                                                                                    |
//|   N - переменный набор входа положительного целого числа к числу переменных.       |
//|                                                                                    |
//|   X - множество длины n. На входе это должно содержать базисную точку              |
//|       для поиска линии. На продукции это содержит x+stp*s.                         |
//|                                                                                    |
//|   F - переменная. На входе это должно содержать ценность f в x.                    |
//|       На продукции Это содержит ценность f в x + stp*s.                            |
//|                                                                                    |
//|   G - множество длины n. На входе это должно содержать градиент f в x.             |
//|       На продукции это содержит градиент f в x + stp*s.                            |
//|                                                                                    |
//|   S - множество входа длины n, который определяет руководство поиска.              |
//|                                                                                    |
//|   STP - неотрицательная переменная. На входе STP содержит начальную оценку         |
//|       удовлетворительного шага. На продукции STP содержит заключительную оценку.   |
//|                                                                                    |
//|   FTOL и GTOL - неотрицательные переменные входа. Завершение происходит, когда     |
//|   достаточное условие уменьшения и направленное производное условие Удовлетворены. |
//|                                                                                    |
//|   XTOL - неотрицательная переменная входа. Завершение происходит, когда            |
//|       относительная ширина интервала неуверенности - в большинстве XTOL.           |
//|                                                                                    |
//|   STPMIN и STPMAX - неотрицательные переменные входа, которые определяют           |
//|       экстремумы - нижние и верхние границы для шага.                              |
//|                                                                                    |
//|   MAXFEV - положительная переменная входа целого числа. Завершение происходит,     |
//|       когда Число звонков fcn - по крайней мере maxfev к концу повторения.         |
//|                                                                                    |
//|   Информация - набор переменной продукции целого числа следующим образом:          |
//|      INFO = 0  неподходящие параметры входа.                                       |
//|                                                                                    |
//|      INFO = 1  достаточное условие уменьшения и направленное                       |
//|                производное условие держатся.                                       |
//|                                                                                    |
//|      INFO = 2  относительная ширина интервала неуверенности - в большинстве XTOL.  |
//|                                                                                    |
//|      INFO = 3  число звонков FCN достигло MAXFEV.                                  |
//|                                                                                    |
//|      INFO = 4  шаг - в ниже связанном STPMIN.                                      |
//|                                                                                    |
//|      INFO = 5  шаг - в связанном STPMAX верхнего.                                  |
//|                                                                                    |
//|      INFO = 6  округляющиеся ошибки предотвращают дальнейшее продвижение.,         |
//|                возможно, быть шага, который удовлетворяет достаточное              |
//|                уменьшение и условия искривления. терпимость может                  |
//|                быть слишком маленькой.                                             |
//|                                                                                    |
//|   NFEV набор переменной продукции целого числа к числу звонков FCN.                |
//|                                                                                    |
//|   WA множество работы длины N.                                                     |
//|                                                                                    |
//|   ARGONNE NATIONAL LABORATORY. MINPACK PROJECT. JUNE 1983                          |
//|   JORGE J. MORE , DAVID J. THUENTE                                                 |
//+------------------------------------------------------------------------------------+

void MCSRCH(int N, 
            double& X[], 
            double& F, 
            double& G[], 
            double& S[], 
            double& STP, 
            int& INFO, 
            int& NFEV, 
            double& WA[], 
            int& State, 
            int& Stage ){
//--------------------------------------------
   double V, P5, P66, ZERO, Stx, Fxm, Dgxm, Sty, Fym, Dgym, Fm, Dgm, Brackt, STmin, STmax, Infoc;
//--------------------------------------------
//   Print("Процедура MCSRCH - поиск шага.");
//--------------------------------------------
// Инициализация
   XTOL = 5.0*MathPow(10.0,-14);
   STPMIN = MathPow(10.0,-20);
   STPMAX = MathPow(10.0,20);

   P5 = 0.5;
   P66 = 0.66;
   LBFGSState[State,XTRAPF,0,0] = 4.0;
   ZERO = 0.0;
//--------------------------------------------
// Главный цикл
   while( True ){
//--------------------------------------------
      if( Stage==0 ) {
// Следующий
         Stage = 2.0;
         continue;      // GoTo Cont_1
      } // if
//--------------------------------------------
      if( Stage==2 ) {
         LBFGSState[State,INFOC,0,0] = 1.0;
         INFO = 0.0;
//--------------------------------------------
// Проверка параметры входа для ошибок.
         if( N<=0 || STP<=0 || FTOL<0 || GTOL<ZERO || XTOL<ZERO || STPMIN<ZERO || STPMAX<STPMIN || MAXFEV<=0 ) {
            Stage = 0.0;
            return(0);      // Exit Sub
         } // if
//--------------------------------------------
// Вычислите начальный градиент в руководстве поиска и проверьте, что S - руководство спуска.
         V = 0.0;
         for( int i=0; i<N; i++)
            V += G[i]*S[i];
//--------------------------------------------
         LBFGSState[State,DGINIT,0,0] = V;
         if( LBFGSState[State,DGINIT,0,0]>=0 ) {
            Stage = 0.0;
            return(0);      // Exit Sub
         } // if
//--------------------------------------------
// Инициализация локальных переменных
         LBFGSState[State,LBF_BRACKT,0,0] = 0.0;
         LBFGSState[State,STAGE1,0,0] = 1.0;
         NFEV = 0.0;
         LBFGSState[State,FINIT,0,0] = F;
         LBFGSState[State,DGTEST,0,0] = FTOL*LBFGSState[State,DGINIT,0,0];
         LBFGSState[State,WIDTH,0,0] = STPMAX-STPMIN;
         LBFGSState[State,WIDTH1,0,0] = LBFGSState[State,WIDTH,0,0]/P5;
//--------------------------------------------
         for( i=0; i<N; i++)
            WA[i] = X[i];
//--------------------------------------------
// Переменные STX, FX, DGX содержат ценности шага, функции, 
// и направленной производной в лучшем шаге. Свинарник переменных 
// STY, FY, DGY содержат ценность шага, функции, и производной в 
// другой конечной точке интервала неуверенности. переменных 
// STP, F, DG дециграмм содержит ценности шага, функции, 
// и производной в текущем шаге. 
         LBFGSState[State,LBF_STX,0,0] = 0.0;
         LBFGSState[State,LBF_FX,0,0] = LBFGSState[State,FINIT,0,0];
         LBFGSState[State,DGX,0,0] = LBFGSState[State,DGINIT,0,0];
         LBFGSState[State,LBF_STY,0,0] = 0.0;
         LBFGSState[State,LBF_FY,0,0] = LBFGSState[State,FINIT,0,0];
         LBFGSState[State,DGY,0,0] = LBFGSState[State,DGINIT,0,0];
//--------------------------------------------
// Следующий
         Stage = 3.0;
         continue;      // GoTo Cont_1
      } // if
//--------------------------------------------
      if( Stage==3 ) {
//--------------------------------------------
//     Начало повторения.
//     Установка минимума и максимума шага, чтобы соответствовать существующему интервалу неуверенности.
         if( LBFGSState[State,LBF_BRACKT,0,0]>0 )
            if( LBFGSState[State,LBF_STX,0,0]<LBFGSState[State,LBF_STY,0,0] ) {
               LBFGSState[State,LBF_STMIN,0,0] = LBFGSState[State,LBF_STX,0,0];
               LBFGSState[State,LBF_STMAX,0,0] = LBFGSState[State,LBF_STY,0,0];
            } else{
               LBFGSState[State,LBF_STMIN,0,0] = LBFGSState[State,LBF_STY,0,0];
               LBFGSState[State,LBF_STMAX,0,0] = LBFGSState[State,LBF_STX,0,0];
            } // if
         else {
            LBFGSState[State,LBF_STMIN,0,0] = LBFGSState[State,LBF_STX,0,0];
            LBFGSState[State,LBF_STMAX,0,0] = STP + LBFGSState[State,XTRAPF,0,0]*(STP - LBFGSState[State,LBF_STX,0,0]);
         } // if
//--------------------------------------------
// Вынудите шаг быть в пределах границ STPMAX и STPMIN.
         if( STP>STPMAX )
            STP = STPMAX;
//--------------------------------------------
         if( STP<STPMIN )
            STP = STPMIN;
//--------------------------------------------
//  if(необычное завершение должно произойти) позволяют НТП быть самым низким пунктом, полученным пока.
         if( (LBFGSState[State,LBF_BRACKT,0,0]>0) &&  
            (STP<=LBFGSState[State,LBF_STMIN,0,0] || STP>=LBFGSState[State,LBF_STMAX,0,0]) ||
               (NFEV>=(MAXFEV-1)) ||          
                  (LBFGSState[State,INFOC,0,0]==0) ||      
                     (LBFGSState[State,LBF_BRACKT,0,0]>0) &&        
                        ((LBFGSState[State,LBF_STMAX,0,0] - LBFGSState[State,LBF_STMIN,0,0])<=(XTOL*LBFGSState[State,LBF_STMAX,0,0])) )
            STP = LBFGSState[State,LBF_STX,0,0];
//--------------------------------------------
//  Если необычное завершение должно произойти, тогда позволяют STP быть самым низким пунктом, полученным пока.
         for( i=0; i<N; i++)
            X[i] = WA[i];
//--------------------------------------------
         for( i=0; i<N; i++)
            X[i] += STP*S[i];
//--------------------------------------------
// Следующий
         Stage = 4.0;
         return(0);        // Exit Sub
      } // if
//--------------------------------------------
      if( Stage==4 ) {
         INFO = 0.0;
         NFEV += 1.0;
         V = 0.0;
//--------------------------------------------
         for( i=0; i<N; i++)
            V += G[i]*S[i];
//--------------------------------------------
         LBFGSState[State,DG,0,0] = V;
         LBFGSState[State,FTEST1,0,0] = LBFGSState[State,FINIT,0,0] + STP*LBFGSState[State,DGTEST,0,0];
//--------------------------------------------
// Тест на конвергенцию.
         if( (LBFGSState[State,LBF_BRACKT,0,0]>0) && 
               ((STP<=LBFGSState[State,LBF_STMIN,0,0]) || (STP>=LBFGSState[State,LBF_STMAX,0,0])) || 
                  (LBFGSState[State,INFOC,0,0]==0) )
            INFO = 6.0;
//--------------------------------------------
         if( (STP==STPMAX) && 
               (F<=LBFGSState[State,FTEST1,0,0]) && 
                  (LBFGSState[State,DG,0,0]<=LBFGSState[State,DGTEST,0,0]) )
            INFO = 5.0;
//--------------------------------------------
         if( (STP==STPMIN) && 
               ((F>LBFGSState[State,FTEST1,0,0]) || 
                  (LBFGSState[State,DG,0,0]>=LBFGSState[State,DGTEST,0,0])) )
            INFO = 4.0;
//--------------------------------------------
         if( NFEV>=MAXFEV )
            INFO = 3.0;
//--------------------------------------------
         if( (LBFGSState[State,LBF_BRACKT,0,0]>0) && 
               ((LBFGSState[State,LBF_STMAX,0,0]-LBFGSState[State,LBF_STMIN,0,0])<=(XTOL*LBFGSState[State,LBF_STMAX,0,0])) )
            INFO = 2.0;
//--------------------------------------------
         if( (F<=LBFGSState[State,FTEST1,0,0]) && 
               (MathAbs(LBFGSState[State,DG,0,0])<=-(GTOL*LBFGSState[State,DGINIT,0,0])) )
            INFO = 1.0;
//--------------------------------------------
// Проверка для завершения.
         if( INFO!=0 ) {
            Stage = 0.0;
            return(0); // Exit Sub
         } // if
//--------------------------------------------
//  В первой стадии мы ищем шаг, для которого измененная функция имеет 
//  неположительную ценность и неотрицательную производную.
         if( (LBFGSState[State,STAGE1,0,0]>0) && 
                  (F<=LBFGSState[State,FTEST1,0,0]) && 
                     (LBFGSState[State,DG,0,0] >= (MathMin(FTOL, GTOL)*LBFGSState[State,DGINIT,0,0])) )
            LBFGSState[State,STAGE1,0,0] = 0.0;
//  Измененная функция используется, чтобы предсказать шаг, только 
//  если мы не получили шаг, для которого измененная функция имеет 
//  неположительную ценность функции и неотрицательную производную, 
//  и если более низкая ценность функции была получена, но уменьшение не достаточно.
//--------------------------------------------
         if( (LBFGSState[State,STAGE1,0,0]>0) && (F<=LBFGSState[State,LBF_FX,0,0]) && (F>LBFGSState[State,FTEST1,0,0]) ) {
//   Определите измененную функцию и производные ценности.
            LBFGSState[State,FM,0,0] = F-STP*LBFGSState[State,DGTEST,0,0];
            LBFGSState[State,FXM,0,0] = LBFGSState[State,LBF_FX,0,0] - LBFGSState[State,LBF_STX,0,0]*LBFGSState[State,DGTEST,0,0];
            LBFGSState[State,FYM,0,0] = LBFGSState[State,LBF_FY,0,0] - LBFGSState[State,LBF_STY,0,0]*LBFGSState[State,DGTEST,0,0];
            LBFGSState[State,DGM,0,0] = LBFGSState[State,DG,0,0] - LBFGSState[State,DGTEST,0,0];
            LBFGSState[State,DGXM,0,0] = LBFGSState[State,DGX,0,0] - LBFGSState[State,DGTEST,0,0];
            LBFGSState[State,DGYM,0,0] = LBFGSState[State,DGY,0,0] - LBFGSState[State,DGTEST,0,0];
//--------------------------------------------
//  Назовите cstep, чтобы обновить интервал неуверенности и вычислить новый шаг.
            Stx = (LBFGSState[State,LBF_STX,0,0]);
            Fxm = (LBFGSState[State,FXM,0,0]);
            Dgxm = (LBFGSState[State,DGXM,0,0]);
            Sty = (LBFGSState[State,LBF_STY,0,0]);
            Fym = (LBFGSState[State,FYM,0,0]);
            Dgym = (LBFGSState[State,DGYM,0,0]);
            Fm = (LBFGSState[State,FM,0,0]);
            Dgm = (LBFGSState[State,DGM,0,0]);
            Brackt = (LBFGSState[State,LBF_BRACKT,0,0]);
            STmin = (LBFGSState[State,LBF_STMIN,0,0]);
            STmax = (LBFGSState[State,LBF_STMAX,0,0]);
            Infoc = (LBFGSState[State,INFOC,0,0]);
//--------------------------------------------
            MCSTEP(Stx, Fxm, Dgxm, Sty, Fym, Dgym, STP, Fm, Dgm, Brackt, STmin, STmax, Infoc);
//--------------------------------------------
            LBFGSState[State,LBF_STX,0,0] = Stx;
            LBFGSState[State,FXM,0,0] = Fxm;
            LBFGSState[State,DGXM,0,0] = Dgxm;
            LBFGSState[State,LBF_STY,0,0] = Sty;
            LBFGSState[State,FYM,0,0] = Fym;
            LBFGSState[State,DGYM,0,0] = Dgym;
            LBFGSState[State,FM,0,0] = Fm;
            LBFGSState[State,DGM,0,0] = Dgm;
            LBFGSState[State,LBF_BRACKT,0,0] = Brackt;
            LBFGSState[State,LBF_STMIN,0,0] = STmin;
            LBFGSState[State,LBF_STMAX,0,0] = STmax;
            LBFGSState[State,INFOC,0,0] = Infoc;
//--------------------------------------------
//  Перезагрузите функцию и ценности градиента для F.
            LBFGSState[State,LBF_FX,0,0] = LBFGSState[State,FXM,0,0] + LBFGSState[State,LBF_STX,0,0]*LBFGSState[State,DGTEST,0,0];
            LBFGSState[State,LBF_FY,0,0] = LBFGSState[State,FYM,0,0] + LBFGSState[State,LBF_STY,0,0]*LBFGSState[State,DGTEST,0,0];
            LBFGSState[State,DGX,0,0] = LBFGSState[State,DGXM,0,0] + LBFGSState[State,DGTEST,0,0];
            LBFGSState[State,DGY,0,0] = LBFGSState[State,DGYM,0,0] + LBFGSState[State,DGTEST,0,0];
//--------------------------------------------
         } else {
//--------------------------------------------
//    Назовите mcstep, чтобы обновить интервал неуверенности и вычислить новый шаг.
            Stx = (LBFGSState[State,LBF_STX,0,0]);
            Fxm = (LBFGSState[State,LBF_FX,0,0]);
            Dgxm = (LBFGSState[State,DGX,0,0]);
            Sty = (LBFGSState[State,LBF_STY,0,0]);
            Fym = (LBFGSState[State,LBF_FY,0,0]);
            Dgym = (LBFGSState[State,DGY,0,0]);
            Dgm = (LBFGSState[State,DG,0,0]);
            Brackt = (LBFGSState[State,LBF_BRACKT,0,0]);
            STmin = (LBFGSState[State,LBF_STMIN,0,0]);
            STmax = (LBFGSState[State,LBF_STMAX,0,0]);
            Infoc = (LBFGSState[State,INFOC,0,0]);
//--------------------------------------------
            MCSTEP(Stx, Fxm, Dgxm, Sty, Fym, Dgym, STP, F, Dgm, Brackt, STmin, STmax, Infoc);
//--------------------------------------------
            LBFGSState[State,LBF_STX,0,0] = Stx;
            LBFGSState[State,LBF_FX,0,0] = Fxm;
            LBFGSState[State,DGX,0,0] = Dgxm;
            LBFGSState[State,LBF_STY,0,0] = Sty;
            LBFGSState[State,LBF_FY,0,0] = Fym;
            LBFGSState[State,DGY,0,0] = Dgym;
            LBFGSState[State,DG,0,0] = Dgm;
            LBFGSState[State,LBF_BRACKT,0,0] = Brackt;
            LBFGSState[State,LBF_STMIN,0,0] = STmin;
            LBFGSState[State,LBF_STMAX,0,0] = STmax;
            LBFGSState[State,INFOC,0,0] = Infoc;
         }
//--------------------------------------------
//    Вызовите достаточное уменьшение в размере интервала неуверенности.
         if( LBFGSState[State,LBF_BRACKT,0,0]>0 ){
            if( MathAbs(LBFGSState[State,LBF_STY,0,0]-LBFGSState[State,LBF_STX,0,0]) >= (P66*LBFGSState[State,WIDTH1,0,0]) )
               STP = LBFGSState[State,LBF_STX,0,0]+P5*(LBFGSState[State,LBF_STY,0,0]-LBFGSState[State,LBF_STX,0,0]);
            LBFGSState[State,WIDTH1,0,0] = LBFGSState[State,WIDTH,0,0];
            LBFGSState[State,WIDTH,0,0] = MathAbs(LBFGSState[State,LBF_STY,0,0] - LBFGSState[State,LBF_STX,0,0]);
         }  // if
//--------------------------------------------
//  NEXT.
         Stage = 3.0;
         continue;      // GoTo Cont_1
      }
//--------------------------------------------
// Cont_1:
   }    // Loop
//--------------------------------------------
}

//+---------------------------------------------------------------------------------+
//|  Цель MCSTEP состоит в том, чтобы вычислить гарантированный шаг                 |
//|  для LINESEARCH и обновлять интервал неуверенности для MINIMIZER функции.       |
//|                                                                                 |
//|  Параметр STX содержит шаг с наименьшим количеством ценности функции.           |
//|  STP параметр содержит текущий шаг. Предполагается, что производная             |
//|  в STX отрицательна в направлении шага. Если BRACKT установлен истинный         |
//|  тогда, MINIMIZER был заключен в скобки в интервале неуверенности               |
//|  с конечными точками STX и STY.                                                 |
//|                                                                                 |
//|  Утверждение подпрограммы                                                       |
//|                                                                                 |
//|  Подпрограмма MCSTEP(STX,FX,DX,STY,FY,DY,STP,FP,DP,BRACKT,STPMIN,STPMAX,INFO)   |
//|                                                                                 |
//|  Где:                                                                           |
//|                                                                                 |
//|    STX, FX, и DX - переменные, которые определяют шаг, функцию,                 |
//|         и производную в лучшем шаге, полученном пока. Производная               |
//|         должна быть отрицательной в направлении шага, то есть, DX и STP-STX     |
//|         должны иметь противоположные признаки.                                  |
//|         На продукции эти параметры обновлены соответственно.                    |
//|                                                                                 |
//|    STY, FY, и DY  - переменные, которые определяют шаг, функцию,                |
//|         и производную в другой конечной точке интервала неуверенности.          |
//|         На продукции эти параметры обновлены соответственно.                    |
//|                                                                                 |
//|    STP, FP, и DP - переменные, которые определяют шаг, функцию, и               |
//|         производную в текущем шаге. Если BRACKT установлен истинный             |
//|         тогда на STP входа, должен быть между STX и STY.                        |
//|         На продукции STP собирается новый шаг.                                  |
//|                                                                                 |
//|    BRACKT - логическая переменная, которая определяет, был ли MINIMIZER         |
//|         заключен в скобки. Если minimizer не был заключен в скобки тогда        |
//|         на входе BRACKT, должен быть установлен ложный. Если MINIMIZER          |
//|         заключен в скобки тогда на продукции BRACKT, установлен истинный.       |
//|                                                                                 |
//|    STPMIN и STPMAX - переменные входа, которые определяют более низкие          |
//|         и верхние границы для шага.                                             |
//|                                                                                 |
//|    INFO Информация - набор переменной продукции целого числа следующим образом: |
//|         Если INFO = 1,2,3,4,5, то шаг был вычислен согласно одному              |
//|         из этих пяти случаев ниже. Иначе INFO = 0,                              |
//|         и это указывает неподходящие параметры входа.                           |
//|                                                                                 |
//|     Подпрограммы Call                                                           |
//|                                                                                 |
//|       Поставляемый ФОРТРАНом ... ABS,MAX,MIN,SQRT                               |
//|                                                                                 |
//|     ARGONNE NATIONAL LABORATORY. MINPACK PROJECT. JUNE 1983                     |
//|     JORGE J. MORE, DAVID J. THUENTE                                             |
//+---------------------------------------------------------------------------------+

void MCSTEP(double& STX, 
            double& FX, 
            double& DX, 
            double& STY, 
            double& FY, 
            double& DY, 
            double& STP, 
            double& FP, 
            double& DP, 
            double& BRACKT, 
            double& STMIN, 
            double& STMAX, 
            double& INFO ){
//--------------------------------------------
//   Print("Процедура MCSTEP - минимальный шаг.");
//--------------------------------------------
   bool BOUND;
   double GAMMA, P, Q, R, S, SGND, STPC, STPF, STPQ, THETA;
//--------------------------------------------
   INFO = 0.0;
//--------------------------------------------
// Проверьте параметры входа для ошибок.
   if( (BRACKT>0) && ((STP<=MathMin(STX, STY)) || (STP>=MathMax(STX, STY))) || (DX*(STP-STX)>=0) || (STMAX<STMIN) )
      return(0);     // Exit Sub
//--------------------------------------------
// Определите, имеют ли производные противоположный признак.
   SGND = DP*(DX/MathAbs(DX));
//--------------------------------------------
// Первый случай. 
// Более высокая ценность функции. 
// Минимум заключен в скобки. Если кубический шаг ближе к STX чем квадратный шаг, 
// кубический шаг сделан, еще среднее число кубических и квадратных шагов взято.
   if( FP>FX ) {
      INFO = 1.0;
      BOUND = True;
      THETA = 3*(FX-FP)/(STP-STX)+DX+DP;
      S = MathMax(MathAbs(THETA), MathMax(MathAbs(DX), MathAbs(DP)));
      GAMMA = S*MathSqrt(MathPow((THETA/S),2)-DX/S*(DP/S));         // (Square(THETA/S)-DX/S*(DP/S));
//--------------------------------------------
      if( STP<STX )
         GAMMA = -GAMMA;
//--------------------------------------------
      P = GAMMA-DX+THETA;
      Q = GAMMA-DX+GAMMA+DP;
      R = P/Q;
      STPC = STX+R*(STP-STX);
      STPQ = STX+DX/((FX-FP)/(STP-STX)+DX)/2*(STP-STX);
//--------------------------------------------
      if( MathAbs(STPC-STX)<MathAbs(STPQ-STX) )
         STPF = STPC;
      else
         STPF = STPC+(STPQ-STPC)/2.0;
      BRACKT = 1.0;
//--------------------------------------------
   } else {
      if( SGND<0 ) {
//--------------------------------------------
// Второй случай. 
// Более низкая функция оценивает и производные противоположного признака. 
// Минимум заключен в скобки. Если кубический шаг ближе к STX чем квадратный (секущий) шаг, 
// кубический шаг сделан, еще квадратный шаг сделан.
         INFO = 2.0;
         BOUND = False;
         THETA = 3*(FX-FP)/(STP-STX)+DX+DP;
         S = MathMax(MathAbs(THETA), MathMax(MathAbs(DX), MathAbs(DP)));
         GAMMA = S*MathSqrt(MathPow((THETA/S),2)-DX/S*(DP/S));            // Square(THETA/S)-DX/S*(DP/S));
//--------------------------------------------
         if( STP>STX )
            GAMMA = -GAMMA;
//--------------------------------------------
         P = GAMMA-DP+THETA;
         Q = GAMMA-DP+GAMMA+DX;
         R = P/Q;
         STPC = STP+R*(STX-STP);
         STPQ = STP+DP/(DP-DX)*(STX-STP);
//--------------------------------------------
         if( MathAbs(STPC-STP)>MathAbs(STPQ-STP) )
            STPF = STPC;
         else
            STPF = STPQ;
         BRACKT = 1.0;
//--------------------------------------------
      } else {
         if( MathAbs(DP)<MathAbs(DX) ) {
//--------------------------------------------
//  Третий случай. 
//  Более низкая ценность функции, производные того же самого признака, 
//  и величины производных уменьшений. Кубический шаг только используется, 
//  если кубическое имеет тенденцию к бесконечности в направлении шага или 
//  если минимум кубического - вне STP. Иначе кубический шаг определен, 
//  чтобы быть или stpmin или stpmax. Квадратный (секущий) шаг также вычислен 
//  и если минимум заключен в скобки тогда, шаг, самый близкий к STX сделан, еще шаг дальше всего далеко сделан.
            INFO = 3.0;
            BOUND = True;
            THETA = 3*(FX-FP)/(STP-STX)+DX+DP;
            S = MathMax(MathAbs(THETA), MathMax(MathAbs(DX), MathAbs(DP)));
// В случае возникновения GAMMA=0, 
// если кубическое не имеет тенденцию к бесконечности в направлении шага.
            GAMMA = S*MathSqrt(MathMax(0, MathPow((THETA/S),2)-DX/S*(DP/S)));    //Square(THETA/S)-DX/S*(DP/S)));
//--------------------------------------------
            if( STP>STX )
               GAMMA = -GAMMA;
            P = GAMMA-DP+THETA;
            Q = GAMMA+(DX-DP)+GAMMA;
            R = P/Q;
//--------------------------------------------
            if( R<0 && GAMMA!=0 )
               STPC = STP+R*(STX-STP);
            else
//--------------------------------------------
               if( STP>STX )
                  STPC = STMAX;
//--------------------------------------------
               else
                  STPC = STMIN;
//--------------------------------------------
            STPQ = STP+DP/(DP-DX)*(STX-STP);
//--------------------------------------------
            if( BRACKT>0 )
//--------------------------------------------
               if( MathAbs(STP-STPC)<MathAbs(STP-STPQ) )
                  STPF = STPC;
               else
                  STPF = STPQ;
//--------------------------------------------
            else
               if( MathAbs(STP-STPC)>MathAbs(STP-STPQ) )
                  STPF = STPC;
               else
                  STPF = STPQ;
//--------------------------------------------
         } else {
//--------------------------------------------
//  Четвертый случай. 
//  Более низкая ценность функции, производные того же самого признака, 
//  и величины производной не уменьшается. Если минимум не заключен в скобки, 
//  шаг является или stpmin или stpmax, еще кубический шаг сделан.
            INFO = 4.0;
            BOUND = False;
//--------------------------------------------
            if( BRACKT>0 ) {
               THETA = 3*(FP-FY)/(STY-STP)+DY+DP;
               S = MathMax(MathAbs(THETA), MathMax(MathAbs(DY), MathAbs(DP)));
               GAMMA = S*MathSqrt(MathPow((THETA/S),2)-DY/S*(DP/S));        // Square(THETA/S)-DY/S*(DP/S));
//--------------------------------------------
               if( STP>STY )
                  GAMMA = -GAMMA;
//--------------------------------------------
               P = GAMMA-DP+THETA;
               Q = GAMMA-DP+GAMMA+DY;
               R = P/Q;
               STPC = STP+R*(STY-STP);
               STPF = STPC;
//--------------------------------------------
            } else
//--------------------------------------------
               if( STP>STX )
                  STPF = STMAX;
               else
                  STPF = STMIN;
//--------------------------------------------
         }
//--------------------------------------------
      }
//--------------------------------------------
   }
//--------------------------------------------
// Обновление интервала неуверенности. 
// Это обновление не зависит от нового шага или анализа случая выше.
   if( FP>FX ) {
      STY = STP;
      FY = FP;
      DY = DP;
//--------------------------------------------
   } else {
      if( SGND<0.0 ) {
         STY = STX;
         FY = FX;
         DY = DX;
      }
//--------------------------------------------
      STX = STP;
      FX = FP;
      DX = DP;
//--------------------------------------------
   }
//--------------------------------------------
// Вычислить новый шаг и сохранить
   STPF = MathMin(STMAX, STPF);
   STPF = MathMax(STMIN, STPF);
   STP = STPF;
//--------------------------------------------
   if( BRACKT>0 && BOUND )
      if( STY>STX )
         STP = MathMin(STX+0.66*(STY-STX), STP);
//--------------------------------------------
      else
         STP = MathMax(STX+0.66*(STY-STX), STP);
//--------------------------------------------
}

//+------------------------------------------------------------------------------+
//|  Получение функции ошибки на тестовом множестве (функция - сумма квадратов)  |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      Network -   структура, задающая обученную/необученную нейронную  сеть.  |
//|                  структура передается по ссылке  и  модифицируется  в  ходе  |
//|                  работы (т.к. её поля используются для  хранения  временных  |
//|                  переменных).                                                |
//|      XY      -   тестовое множество.                                         |
//|                  массив размером [0..SSize-1,0..NIn+NOut-1].                 |
//|                  первые   NIn   столбцов   содержат   входные   данные,      |
//|                  последующие NOut столбцов  содержат желаемые  значения      |
//|                  выходов сети.                                               |
//|      SSize   -   размер тестового множества                                  |
//|                                                                              |
//|  Результат:                                                                  |
//|      функция ошибки, SUM(sqr(y[i]-desy[i])/2,i)                              |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+
double MLPError(  int Network,      // - структура НС                               - (входной параметр)
                  double& XY[][],   // - тестовое множество (результаты опытов)     - (входной параметр)
                  int SSize ){      // - размер тестового множества (кол-во опытов) - (входной параметр)
//--------------------------------------------
   int i1_, NIn, NOut, WCount;
   double Result, E, X[], Y[];
//--------------------------------------------
//   Print("Процедура MLPError - ошибка результата.");
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);
//--------------------------------------------
   ArrayResize(X,NIn);
   ArrayResize(Y,NOut);
   Result = 0.0;
   for( int i=0; i<SSize; i++){
//--------------------------------------------
        for( int j=0; j<NIn; j++){
            MultiLayerPerceptron[Network,MLP_X,j,0] = XY[i,j];
            X[j] = MultiLayerPerceptron[Network,MLP_X,j,0];
        }
//--------------------------------------------
        for( j=0; j<NOut; j++)
            Y[j] = MultiLayerPerceptron[Network,MLP_Y,j,0];
//--------------------------------------------
        MLPProcess(  Network, X, Y);
//--------------------------------------------
        for( j=0; j<NIn; j++)
            MultiLayerPerceptron[Network,MLP_X,j,0] = X[j];
//--------------------------------------------
        for( j=0; j<NOut; j++)
            MultiLayerPerceptron[Network,MLP_Y,j,0] = Y[j];
//--------------------------------------------
        i1_ = (NIn) - (0);
//--------------------------------------------
        for( j=0; j<NOut; j++)
            MultiLayerPerceptron[Network,MLP_X,j,0] -= XY[i,j+i1_];
//--------------------------------------------
        E = 0.0;
        for( j=0; j<NOut; j++)
            E += MultiLayerPerceptron[Network,MLP_Y,j,0]*MultiLayerPerceptron[Network,MLP_Y,j,0];
        Result += E/2.0;
//--------------------------------------------
    } // for I
//--------------------------------------------
    return(Result);
//--------------------------------------------
}


//+---------------------------------------------------------------------------------+
//|   Получение естественной функции ошибки на тестовом множестве                   |
//|                                                                                 |
//|   Входные параметры:                                                            |
//|       Network -   структура, задающая обученную/необученную нейронную  сеть.    |
//|                   структура передается по ссылке  и  модифицируется  в  ходе    |
//|                   работы (т.к. её поля используются для  хранения  временных    |
//|                   переменных).                                                  |
//|       XY      -   тестовое множество.                                           |
//|                   массив размером [0..SSize-1,0..NIn+NOut-1].                   |
//|                   первые   NIn   столбцов   содержат   входные   данные,        |
//|                   последующие NOut столбцов  содержат желаемые  значения        |
//|                   выходов сети.                                                 |
//|       SSize   -   размер тестового множества                                    |
//|                                                                                 |
//|   Результат:                                                                    |
//|       естественная функция ошибки.  Сумма  квадратов  отклонений  для  сети-    |
//|       регрессора   (т.е.  сети  без  softmax-нормализации  выходов),  кросс-    |
//|       энтропия для сети-классификатора (с softmax-нормализацией выходов).       |
//|                                                                                 |
//|     -- ALGLIB --                                                                |
//|        Copyright 04.11.2007 by Bochkanov Sergey                                 |
//+---------------------------------------------------------------------------------+

double MLPErrorN( int Network,      // - структура НС                               - (входной параметр)
                  double& XY[][],   // - тестовое множество (результаты опытов)     - (входной параметр)
                  int SSize ){      // - размер тестового множества (кол-во опытов) - (входной параметр)
//--------------------------------------------
   int i1_, NIn, NOut, WCount;
   double Result, E, T, Z, X[], Y[];
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);
//--------------------------------------------
//   Print("Процедура MLPErrorN - ошибка результата.");
//--------------------------------------------
   ArrayResize(X,NIn);
   ArrayResize(Y,NOut);
   Result = 0.0;
   for( int i=0; i<SSize; i++){
//--------------------------------------------
// процесс Вектор
      for( int j=0; j<NIn; j++){
         MultiLayerPerceptron[Network,MLP_X,j,0] = (XY[i,j]);
            X[j] = MultiLayerPerceptron[Network,MLP_X,j,0];
        }
//--------------------------------------------
        for( j=0; j<NOut; j++)
            Y[j] = MultiLayerPerceptron[Network,MLP_Y,j,0];
//--------------------------------------------
        MLPProcess(  Network, X, Y);
//--------------------------------------------
        for( j=0; j<NIn; j++)
            MultiLayerPerceptron[Network,MLP_X,j,0] = X[j];
//--------------------------------------------
        for( j=0; j<NOut; j++)
            MultiLayerPerceptron[Network,MLP_Y,j,0] = Y[j];
//--------------------------------------------
// Update error function
      if( MultiLayerPerceptron[Network,StructInfo,6,0]==0 ) {
// Функция вычисления ошибки по наименьшим квадратам
         i1_ = (NIn) - (0);
//--------------------------------------------
         for( j=0; j<NOut; j++)
            MultiLayerPerceptron[Network,MLP_Y,j,0] -= XY[i,j+i1_];
//--------------------------------------------
         E = 0.0;
         for( j=0; j<NOut; j++)
            E += MultiLayerPerceptron[Network,MLP_Y,j,0]*MultiLayerPerceptron[Network,MLP_Y,j,0];
//--------------------------------------------
            Result += E/2.0;
      } else {
//--------------------------------------------
// Функция вычисления ошибки поперечной энтропии:
// E = trn * ln (trn/out) = trn * ln(r)
// Отметьте, что TRN и OUT ожидаются в пределах [0,1]
// но мы не полагаемся на это (код более здравый).
//--------------------------------------------
         for( j=0; j<NOut; j++){
            T = XY[i,NIn+j];
            Z = MultiLayerPerceptron[Network,MLP_Y,j,0];
            Result = Result+SafeCrossEntropy(T, Z);
         } // for J
//--------------------------------------------
      } // if
//--------------------------------------------
   } // I
   return(Result);
//--------------------------------------------
}

//+---------------------------------------------------------------------------------+
//|  Обработка вектора                                                              |
//|                                                                                 |
//|  Входные параметры:                                                             |
//|      Network -   структура, задающая обученную/необученную нейронную  сеть.     |
//|                  структура передается по ссылке  и  модифицируется  в  ходе     |
//|                  работы (т.к. её поля используются для  хранения  временных     |
//|                  переменных).                                                   |
//|      X       -   входной вектор,  по размеру должен быть равен числу входов     |
//|                  сети, [0..NIn-1].                                              |
//|                                                                                 |
//|  Выходные параметры:                                                            |
//|      Y       -   результат работы сети. Подпрограмма не выделяет память под     |
//|                  этот вектор, это обязанность того, кто вызывает подпрограмму,  |
//|                  выделить память под результат. Массив должен иметь  размер     |
//|                  [0..NOut-1].                                                   |
//|                                                                                 |
//|    -- ALGLIB --                                                                 |
//|       Copyright 04.11.2007 by Bochkanov Sergey                                  |
//+---------------------------------------------------------------------------------+
void MLPProcess(  int Network, 
                  double& X[], 
                  double& Y[] ){
//--------------------------------------------
   double SI[], WE[], CM[], CS[], Neur[], DFD[];
   int NIn, NOut, WCount;
//--------------------------------------------
//   Print("Процедура MLPProcess - Обработка вектора.");
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);
//--------------------------------------------
   MLPInternalProcessVector(Network,X,Y);
//--------------------------------------------
}


//+--------------------------------------------------------------------------------+
//|                                                                                |
//|  Внутренняя подпрограмма.                                                      |
//|                                                                                |
//+--------------------------------------------------------------------------------+
void MLPInternalProcessVector(int Network, 
                              double& X[], 
                              double& Y[] ){
//--------------------------------------------
//   Print("Процедура нахождение вектора.");
//--------------------------------------------
   int i, i1_, Ind, N1, N2, W1, W2, NTotal, NIn, NOut, IStart, Offs;
   double NET, E, F, DF, D2F, MX;
   bool PErr;
//--------------------------------------------
// Чтерние геометрии сети
   NIn = MultiLayerPerceptron[Network,StructInfo,1,0];
   NOut = MultiLayerPerceptron[Network,StructInfo,2,0];
   NTotal = MultiLayerPerceptron[Network,StructInfo,3,0];
   IStart = MultiLayerPerceptron[Network,StructInfo,5,0];
//--------------------------------------------
// Вход стандартизированние и включение сети
   for( i=0; i<NIn; i++)
      if( MultiLayerPerceptron[Network,ColumnSigmas,i,0]!=0 ){
         MultiLayerPerceptron[Network,Neurons,i,0] = 
                     (X[i]-MultiLayerPerceptron[Network,ColumnMeans,i,0])/MultiLayerPerceptron[Network,ColumnSigmas,i,0];
      } else {
         MultiLayerPerceptron[Network,Neurons,i,0] = X[i]-MultiLayerPerceptron[Network,ColumnMeans,i,0];
      }
//--------------------------------------------
// Сеть процесса
   for( i=0; i<NTotal; i++){
      Offs = IStart+i*NFieldWidth;
//--------------------------------------------
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]>0 ) {
// Функция активации
         Ind = MultiLayerPerceptron[Network,StructInfo,(Offs+2),0];
         MLPActivationFunction(  MultiLayerPerceptron[Network,Neurons,Ind,0], MultiLayerPerceptron[Network,StructInfo,Offs,0], F, DF, D2F);
         MultiLayerPerceptron[Network,Neurons,i,0] = F;
         MultiLayerPerceptron[Network,DFDNET,i,0] = DF;
      } // if
//--------------------------------------------
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==0 ) {
// Адаптивный сумматор
         N1 = MultiLayerPerceptron[Network,StructInfo,(Offs+2),0];
         N2 = N1+MultiLayerPerceptron[Network,StructInfo,(Offs+1),0]-1.0;
         W1 = MultiLayerPerceptron[Network,StructInfo,(Offs+3),0];
         W2 = W1+MultiLayerPerceptron[Network,StructInfo,(Offs+1),0]-1.0;
         i1_ = (N1)-(W1);
         NET = 0.0;
//--------------------------------------------
         for( int j=W1; j<=W2; j++){
            NET += MultiLayerPerceptron[Network,Weights,j,0]*MultiLayerPerceptron[Network,Neurons,(j+i1_),0];
         }
//--------------------------------------------
         MultiLayerPerceptron[Network,Neurons,i,0] = NET;
         MultiLayerPerceptron[Network,DFDNET,i,0] = 1.0;
      } // if
//--------------------------------------------
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]<0 ) {
         PErr = True;
//--------------------------------------------
// нейрон входа, оставленный неизменный
         if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==-2 ){
            PErr = False;
         }
//--------------------------------------------
// "-1" neuron
         if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==-3 ) {
            MultiLayerPerceptron[Network,Neurons,i,0] = -1.0;
            PErr = False;
         } // if
//--------------------------------------------
// "0" neuron
         if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==-4 ) {
            MultiLayerPerceptron[Network,Neurons,i,0] = 0.0;
            PErr = False;
         } // if
//--------------------------------------------
         if(PErr) {
            Alert("MLPInternalProcessVector: внутренняя ошибка - неизвестный тип нейрона!");
            return(0);
         } // if
      } // if
//--------------------------------------------
   } // for I
//--------------------------------------------
// Извлечение результата
   i1_ = (NTotal-NOut);
   for( i=0; i<NOut; i++)
      Y[i] = MultiLayerPerceptron[Network,Neurons,(i+i1_),0];
//--------------------------------------------
   if( !(MultiLayerPerceptron[Network,StructInfo,6,0]==0 || MultiLayerPerceptron[Network,StructInfo,6,0]==1) ){
      Alert("MLPInternalProcessVector: неизвестный тип нормализации!");
      return(0);
   } // if
//--------------------------------------------
// Постобработка Мax или стандартизация если нужно
   if( MultiLayerPerceptron[Network,StructInfo,6,0]==1 ) {
// Программа Мax
      MX = Y[0];
//--------------------------------------------
      for( i=1; i<NOut; i++)
         MX = MathMax(MX, Y[i]);
//--------------------------------------------
      NET = 0.0;
      for( i=0; i<NOut; i++){
         Y[i] = MathExp(Y[i]-MX);
         NET += Y[i];
      } // for I
//--------------------------------------------
      for( i=0; i<NOut; i++)
         Y[i] /= NET;
   } else {
//--------------------------------------------
// Стандартизация
      for( i=0; i<NOut; i++)
         Y[i] = Y[i]*MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0]+MultiLayerPerceptron[Network,ColumnMeans,(NIn+i),0];
//--------------------------------------------
   }
//--------------------------------------------
}

//+------------------------------------------------------------------------------+
//|   Получение ошибки классификации на тестовом множестве                       |
//|                                                                              |
//|   Входные параметры:                                                         |
//|       Network -   структура, задающая обученную/необученную нейронную  сеть. |
//|                   структура передается по ссылке  и  модифицируется  в  ходе |
//|                   работы (т.к. её поля используются для  хранения  временных |
//|                   переменных).                                               |
//|       XY      -   тестовое множество.                                        |
//|                   массив размером [0..SSize-1,0..NIn+NOut-1].                |
//|                   первые   NIn   столбцов   содержат   входные   данные,     |
//|                   последующие NOut столбцов  содержат желаемые  значения     |
//|                   выходов сети.                                              |
//|       SSize   -   размер тестового множества                                 |
//|                                                                              |
//|   Результат:                                                                 |
//|       ошибка классификации (число неправильно классифицированных случаев)    |
//|                                                                              |
//|     -- ALGLIB --                                                             |
//|        Copyright 04.11.2007 by Bochkanov Sergey                              |
//+------------------------------------------------------------------------------+
int MLPClsError(  int Network,      // - структура НС                               - (входной параметр)
                  double& XY[][],   // - тестовое множество (результаты опытов)     - (входной параметр)
                  int SSize ){      // - размер тестового множества (кол-во опытов) - (входной параметр)
//--------------------------------------------
   int NIn, NOut, WCount, Result, NN, NS, NMAX;
   double WorkX[], WorkY[];
//--------------------------------------------
//   Print("Получение ошибки классификации на тестовом множестве.");
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);
//--------------------------------------------
   ArrayResize(WorkX,NIn);   // ReDim WorkX(0 To NIn-1)
   ArrayResize(WorkY,NOut);  // ReDim WorkY(0 To NOut-1)
   Result = 0.0;
//--------------------------------------------
   for( int i=0; i<SSize; i++){
//--------------------------------------------
// Process
      for( int j=0; j<NIn; j++)
         WorkX[j] = XY[i,j];
      MLPProcess(Network, WorkX, WorkY);
//--------------------------------------------
// Версия ответа сети
      NMAX = 0.0;
      for( j=0; j<NOut; j++)
         if( WorkY[j]>WorkY[NMAX] )
            NMAX = j;
      NN = NMAX;
//--------------------------------------------
// Правильный ответ
      NMAX = 0.0;
      for( j=0; j<NOut; j++)
         if( XY[i,NIn+j]>XY[i,NIn+NMAX] )
            NMAX = j;
      NS = NMAX;
//--------------------------------------------
// сравнитесь
      if( NN != NS )
         Result = Result+1.0;
   } // for I
//--------------------------------------------
   return(Result);
}

//+------------------------------------------------------------------+
//|  Копирование структуры MultiLayerPerceptron                      |
//|                                                                  |
//|  Входные параметры:                                              |
//|      Network1    -   сеть-оригинал                               |
//|                                                                  |
//|  Выходные параметры:                                             |
//|      Network2    -   полная копия сети                           |
//|                                                                  |
//|    -- ALGLIB --                                                  |
//|       Copyright 04.11.2007 by Bochkanov Sergey                   |
//+------------------------------------------------------------------+

void MLPCopy( int Network1, int Network2){
   int SSize, NTotal, NIn, NOut, WCount, i_;
//--------------------------------------------
// Выгрузка информации
   SSize = MultiLayerPerceptron[Network1,StructInfo,0,0];
   NIn = MultiLayerPerceptron[Network1,StructInfo,1,0];
   NOut = MultiLayerPerceptron[Network1,StructInfo,2,0];
   NTotal = MultiLayerPerceptron[Network1,StructInfo,3,0];
   WCount = MultiLayerPerceptron[Network1,StructInfo,4,0];
//--------------------------------------------
// Копирование
   for( int i=0; i<SSize; i++)
      MultiLayerPerceptron[Network2,StructInfo,i,0] = MultiLayerPerceptron[Network1,StructInfo,i,0];
   for( i=0; i<WCount; i++)
      MultiLayerPerceptron[Network2,Weights,i,0] = MultiLayerPerceptron[Network1,Weights,i,0];
   for( i=0; i<NIn+NOut; i++)
      MultiLayerPerceptron[Network2,ColumnMeans,i,0] = MultiLayerPerceptron[Network1,ColumnMeans,i,0];
   for( i=0; i<NIn+NOut; i++)
      MultiLayerPerceptron[Network2,ColumnSigmas,i,0] = MultiLayerPerceptron[Network1,ColumnSigmas,i,0];
   for( i=0; i<NTotal; i++)
      MultiLayerPerceptron[Network2,Neurons,i,0] = MultiLayerPerceptron[Network1,Neurons,i,0];
   for( i=0; i<NTotal; i++)
      MultiLayerPerceptron[Network2,DFDNET,i,0] = MultiLayerPerceptron[Network1,DFDNET,i,0];
   for( i=0; i<NIn; i++)
      MultiLayerPerceptron[Network2,MLP_X,i,0] = MultiLayerPerceptron[Network1,MLP_X,i,0];
   for( i=0; i<NOut; i++)
      MultiLayerPerceptron[Network2,MLP_Y,i,0] = MultiLayerPerceptron[Network1,MLP_Y,i,0];
   for( i=0; i<NTotal; i++)
      MultiLayerPerceptron[Network2,DError,i,0] = MultiLayerPerceptron[Network1,DError,i,0];
//--------------------------------------------
}



//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Инициализация  структуры  MultiLayerPerceptron,  как  нейронной   сети   с  |
//|  заданным числом входов и выходов, без скрытых слоев.  В  качестве  функции  |
//|  активации скрытых слоев  используется  гиперболический  тангенс,  выходной  |
//|  слой линейный.                                                              |
//|                                                                              |
//|  По умолчанию инициализированная структура содержит сеть с малыми случайными |
//|  значениями весов и отключенным стандартизатором входов.                     |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+
void MLPCreate0( int NIn,  int NOut, int Network ) {
//--------------------------------------------
     int LayersCount, LastProc, LSizes[], LTypes[], LConnFirst[], LConnLast[];
//--------------------------------------------
// Установка массивов
   LayersCount = 1.0 + 2.0;
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
    AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
    AddBiasedSummatorLayer(NOut, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
    MLPCreate( NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, False, Network);
//--------------------------------------------
Print("Инициирована нейросеть.\nВ качестве функции активации скрытых слоев - гиперболический тангенс, \nвыходной слой линейный. Без скрытых слоев.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|   Аналог MLPCreate0, с одним скрытым слоем                                   |
//|                                                                              |
//|     -- ALGLIB --                                                             |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+
void MLPCreate1(  int& NIn, int& NHid, int& NOut, int& Network ) {
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
    LayersCount = 1.0 + 3.0 + 2.0;
//--------------------------------------------
// Установка массивов
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
   AddInputLayer( NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer( NHid, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(  1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer( NOut, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
   MLPCreate(  NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, False, Network);
//--------------------------------------------
Print("Инициирована нейросеть.\nВ качестве функции активации скрытых слоев - гиперболический тангенс, \nвыходной слой линейный. С одним скрытым слоем.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Аналог MLPCreate0, с двумя скрытыми слоями                                  |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPCreate2(  int& NIn, int& NHid1, int& NHid2, int& NOut, int& Network){
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
// Установка массивов
   LayersCount = 1.0+3.0+3.0+2.0;
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
    AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
    AddBiasedSummatorLayer(NHid1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
    AddActivationLayer( 1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
    AddBiasedSummatorLayer(NHid2, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
    AddActivationLayer( 1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
    AddBiasedSummatorLayer(NOut, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
    MLPCreate( NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, False, Network);
//--------------------------------------------
Print("Инициирована нейросеть.\nВ качестве функции активации скрытых слоев - гиперболический тангенс, \nвыходной слой линейный. С двумя скрытыми слоями.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Инициализация  структуры  MultiLayerPerceptron,  как  нейронной   сети   с  |
//|  заданным числом входов и выходов, без скрытых слоев.  В  качестве  функции  |
//|  активации  скрытых  слоев  используется  гиперболический тангенс, диапазон  |
//|  значений выходного слоя имеет вид:                                          |
//|                                                                              |
//|      (B, +INF), если D>=0                                                    |
//|  или                                                                         |
//|      (-INF, B), если D<0.                                                    |
//|                                                                              |
//|  По умолчанию инициализированная структура содержит сеть с малыми случайными |
//|  значениями весов.                                                           |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 30.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+
void MLPCreateB0( int& NIn, int& NOut, double& B, double& D, int& Network){
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
   LayersCount = 1.0+3.0;
   if( D>=0 )
      D = 1.0;
   else
      D = -1.0;
//--------------------------------------------
// Установка массивов
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
   AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NOut, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(3, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
   MLPCreate(NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, False, Network);
//--------------------------------------------
// Включите изменение/вычисление выходных данных.
   for( int i=NIn; i<(NIn+NOut); i++){
      MultiLayerPerceptron[Network,ColumnMeans,i,0] = B;
      MultiLayerPerceptron[Network,ColumnSigmas,i,0] = D;
    } // for I
//--------------------------------------------
Print("Инициирована нейросеть.\nВ качестве функции активации скрытых слоев - гиперболический тангенс. \nДиапазон значений выходного слоя имеет вид: (B, +INF) если D>=0, или (-INF, B), если D<0. Без скрытых слоев.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Аналог MLPCreateB0, с одним скрытым слоем                                   |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 30.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPCreateB1( int& NIn, int& NHid, int& NOut, double& B, double& D, int& Network){
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
   LayersCount = 1.0+3.0+3.0;
   if( D>=0 )
      D = 1.0;
   else
      D = -1.0;
//--------------------------------------------
// Установка массивов
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
   AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NHid, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NOut, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(3, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
   MLPCreate(NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, False, Network);
//--------------------------------------------
// Включите изменение/вычисление выходных данных.
   for( int i=NIn; i<(NIn+NOut); i++){
      MultiLayerPerceptron[Network,ColumnMeans,i,0] = B;
      MultiLayerPerceptron[Network,ColumnSigmas,i,0] = D;
    } // for I
//--------------------------------------------
Print("Инициирована нейросеть.\nВ качестве функции активации скрытых слоев - гиперболический тангенс. \nДиапазон значений выходного слоя имеет вид: (B, +INF) если D>=0, или (-INF, B), если D<0. С одним скрытым слоем.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Аналог MLPCreateB0, с двумя скрытыми слоями                                 |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 30.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+
void MLPCreateB2( int& NIn, int& NHid1, int& NHid2, int& NOut, double& B, double& D, int& Network){
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
   LayersCount = 1.0+3.0+3.0+3.0;
   if( D>=0 )
      D = 1.0;
   else
      D = -1.0;
//--------------------------------------------
// Установка массивов
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
    // Слои
   AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NHid1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NHid2, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NOut, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(3, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
   MLPCreate(NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, False, Network);
//--------------------------------------------
// Включите изменение/вычисление выходных данных.
   for( int i=NIn; i<(NIn+NOut); i++){
      MultiLayerPerceptron[Network,ColumnMeans,i,0] = B;
      MultiLayerPerceptron[Network,ColumnSigmas,i,0] = D;
    } // for I
//--------------------------------------------
Print("Инициирована нейросеть.\nВ качестве функции активации скрытых слоев - гиперболический тангенс. \nДиапазон значений выходного слоя имеет вид: (B, +INF) если D>=0, или (-INF, B), если D<0. С двумя скрытыми слоями.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Инициализация  структуры  MultiLayerPerceptron,  как  нейронной   сети   с  |
//|  заданным числом входов и выходов, без скрытых слоев.  В  качестве  функции  |
//|  активации  скрытых  слоев  используется  гиперболический тангенс, диапазон  |
//|  значений выходного слоя равен [A,B] (используется гиперболический  тангенс  |
//|  в сочетании со сдвигом/машстабированием).                                   |
//|                                                                              |
//|  По умолчанию инициализированная структура содержит сеть с малыми случайными |
//|  значениями весов.                                                           |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 30.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPCreateR0( int& NIn, int& NOut, double& A, double& B, int& Network){
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
   LayersCount = 1.0+3.0;
//--------------------------------------------
// Установка массивов
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
   AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NOut, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
   MLPCreate(NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, False, Network);
//--------------------------------------------
// Включите изменение/вычисление выходных данных.
   for( int i=NIn; i<(NIn+NOut); i++){
      MultiLayerPerceptron[Network,ColumnMeans,i,0] = 0.5*(A+B);
      MultiLayerPerceptron[Network,ColumnSigmas,i,0] = 0.5*(A-B);
    } // for I
//--------------------------------------------
Print("Инициирована нейросеть.\nВ качестве функции активации скрытых слоев - гиперболический тангенс. \nДиапазон значений выходного слоя равен [A,B] (используется гиперболический тангенс в сочетании со сдвигом/машстабированием). Без скрытых слоев.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Аналог MLPCreateR0, с одним скрытым слоем                                   |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 30.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPCreateR1(int& NIn, int& NHid, int& NOut, double& A, double& B, int& Network){
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
   LayersCount = 1.0+3.0+3.0;
//--------------------------------------------
// Установка массивов
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
   AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NHid, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NOut, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
   MLPCreate(NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, False, Network);
//--------------------------------------------
// Включите изменение/вычисление выходных данных.
   for( int i=NIn; i<(NIn+NOut); i++){
      MultiLayerPerceptron[Network,ColumnMeans,i,0] = 0.5*(A+B);
      MultiLayerPerceptron[Network,ColumnSigmas,i,0] = 0.5*(A-B);
    } // for I
//--------------------------------------------
Print("Инициирована нейросеть.\nВ качестве функции активации скрытых слоев - гиперболический тангенс. \nДиапазон значений выходного слоя равен [A,B] (используется гиперболический тангенс в сочетании со сдвигом/машстабированием). С одним скрытым слоем.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Аналог MLPCreateR0, с двумя скрытыми слоями                                 |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 30.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPCreateR2( int& NIn, int& NHid1, int& NHid2, int& NOut, double& A, double& B, int& Network){
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
   LayersCount = 1.0+3.0+3.0+3.0;
//--------------------------------------------
// Установка массивов
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
   AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NHid1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NHid2, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NOut, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
    // Создать
   MLPCreate(NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, False, Network);
//--------------------------------------------
    // Включите изменение/вычисление выходных данных.
   for( int i=NIn; i<(NIn+NOut); i++){
      MultiLayerPerceptron[Network,ColumnMeans,i,0] = 0.5*(A+B);
      MultiLayerPerceptron[Network,ColumnSigmas,i,0] = 0.5*(A-B);
    } // for I
//--------------------------------------------
Print("Инициирована нейросеть.\nВ качестве функции активации скрытых слоев - гиперболический тангенс. \nДиапазон значений выходного слоя равен [A,B] (используется гиперболический тангенс в сочетании со сдвигом/машстабированием). С двумя скрытыми слоями.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Создание нейронной сети-классификатора без скрытых слоев.                   |
//|                                                                              |
//|  По умолчанию инициализированная структура содержит сеть с малыми случайными |
//|  значениями весов и отключенным стандартизатором входов.                     |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPCreateC0( int& NIn, int& NOut, int& Network){
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
   LayersCount = 1.0+2.0+1.0;
//--------------------------------------------
// Установка массивов
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
   AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NOut-1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddZeroLayer(LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
   MLPCreate(NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, True, Network);
//--------------------------------------------
Print("Инициирована нейросеть.\nНейронная сеть-классификатор. Без скрытых слоев.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Создание нейронной сети-классификатора                                      |
//|                                                                              |
//|  Аналог MLPCreateC0, с одним скрытым слоем                                   |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPCreateC1( int& NIn, int& NHid, int& NOut, int& Network){
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
   LayersCount = 1.0+3.0+2.0+1.0;
//--------------------------------------------
// Установка массивов
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
   AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NHid, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NOut-1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddZeroLayer(LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
   MLPCreate(NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, True, Network);
//--------------------------------------------
Print("Инициирована нейросеть.\nНейронная сеть-классификатор. С одним скрытым слоем.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Создание нейронной сети-классификатора                                      |
//|                                                                              |
//|  Аналог MLPCreateC0, с двумя скрытыми слоями                                 |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPCreateC2( int& NIn, int& NHid1, int& NHid2, int& NOut, int& Network){
//--------------------------------------------
   int LSizes[], LTypes[], LConnFirst[], LConnLast[], LayersCount, LastProc;
//--------------------------------------------
    LayersCount = 1.0+3.0+3.0+2.0+1.0;
//--------------------------------------------
// Установка массивов
   ArrayResize(LSizes,LayersCount);
   ArrayResize(LTypes,LayersCount);
   ArrayResize(LConnFirst,LayersCount);
   ArrayResize(LConnLast,LayersCount);
//--------------------------------------------
// Слои
   AddInputLayer(NIn, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NHid1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NHid2, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddActivationLayer(1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddBiasedSummatorLayer(NOut-1, LSizes, LTypes, LConnFirst, LConnLast, LastProc);
   AddZeroLayer(LSizes, LTypes, LConnFirst, LConnLast, LastProc);
//--------------------------------------------
// Создать
   MLPCreate(NIn, NOut, LSizes, LTypes, LConnFirst, LConnLast, LayersCount, True, Network);
//--------------------------------------------
Print("Инициирована нейросеть.\nНейронная сеть-классификатор. С двумя скрытыми слоями.");
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|                                                                              |
//| Внутренняя подпрограмма: добавление нового слоя входа к сети                 |
//|                                                                              |
//+------------------------------------------------------------------------------+

void AddInputLayer( int& NCount, int& LSizes[], int& LTypes[], int& LConnFirst[], int& LConnLast[], int& LastProc ){
//--------------------------------------------
    LSizes[0] = NCount;
    LTypes[0] = -2.0;
    LConnFirst[0] = 0.0;
    LConnLast[0] = 0.0;
    LastProc = 0.0;
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|                                                                              |
//| Внутренняя подпрограмма: добавление нового summator слоя к сети              |
//|                                                                              |
//+------------------------------------------------------------------------------+
void AddBiasedSummatorLayer( int NCount, int& LSizes[], int& LTypes[], int& LConnFirst[], int& LConnLast[], int& LastProc){
//--------------------------------------------
    LSizes[LastProc+1] = 1.0;
    LTypes[LastProc+1] = -3.0;
    LConnFirst[LastProc+1] = 0.0;
    LConnLast[LastProc+1] = 0.0;
    LSizes[LastProc+2] = NCount;
    LTypes[LastProc+2] = 0.0;
    LConnFirst[LastProc+2] = LastProc;
    LConnLast[LastProc+2] = LastProc+1.0;
    LastProc = LastProc+2.0;
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|                                                                              |
//| Внутренняя подпрограмма: добавление нового summator слоя к сети              |
//|                                                                              |
//+------------------------------------------------------------------------------+
void AddActivationLayer( int FuncType, int& LSizes[], int& LTypes[], int& LConnFirst[], int& LConnLast[], int& LastProc){
//--------------------------------------------
   LSizes[LastProc+1] = LSizes[LastProc];
   LTypes[LastProc+1] = FuncType;
   LConnFirst[LastProc+1] = LastProc;
   LConnLast[LastProc+1] = LastProc;
   LastProc = LastProc+1.0;
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|                                                                              |
//| Внутренняя подпрограмма: добавление нового нулевого слоя к сети              |
//|                                                                              |
//+------------------------------------------------------------------------------+
void AddZeroLayer( int& LSizes[], int& LTypes[], int& LConnFirst[], int& LConnLast[], int& LastProc){
//--------------------------------------------
    LSizes[LastProc+1] = 1.0;
    LTypes[LastProc+1] = -4.0;
    LConnFirst[LastProc+1] = 0.0;
    LConnLast[LastProc+1] = 0.0;
    LastProc = LastProc+1.0;
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|                                                                              |
//| Инициализация  структуры  MultiLayerPerceptron,  как  нейронной   сети   с   |
//| заданной геометрией. Внутренняя подпрограмма.                                |
//|                                                                              |
//|  -- ALGLIB --                                                                |
//|     Copyright 04.11.2007 by Bochkanov Sergey                                 |
//+------------------------------------------------------------------------------+
void MLPCreate(int& NIn, int& NOut, int& LSizes[], int& LTypes[], int& LConnFirst[], 
               int& LConnLast[], int& LayersCount, bool IsClsNet, int Network){
//--------------------------------------------
   int i, SSize, NTotal, WCount, Offs, NProcessed, WAllocated, LocalTemp[], LNFirst[], LNSyn[];
//--------------------------------------------
// Проверка
   for( i=0; i<LayersCount; i++) 
      Network=Network;
//--------------------------------------------
// Построение геометрии сети
   ArrayResize(LNFirst,LayersCount);
   ArrayResize(LNSyn,LayersCount);
   NTotal = 0.0;
   WCount = 0.0;
//--------------------------------------------
   for( i=0; i<LayersCount; i++){
// Анализ связи.
// Этот кодекс должен бросить утверждение в случае неизвестного LTypes[i]
      LNSyn[i] = -1.0;
//--------------------------------------------
      if( LTypes[i]>=0 ) {
         LNSyn[i] = 0.0;
         for( int j=LConnFirst[i]; j<=LConnLast[i]; j++)
            LNSyn[i] = LNSyn[i]+LSizes[j];
      } else
         if( LTypes[i]==-2 || LTypes[i]==-3 || LTypes[i]==-4 )
            LNSyn[i] = 0.0;
//--------------------------------------------
// Другая информация
      LNFirst[i] = NTotal;
      NTotal = NTotal + LSizes[i];
      if( LTypes[i]==0 ) 
         WCount = WCount + LNSyn[i]*LSizes[i];
//--------------------------------------------
   } // for I
//--------------------------------------------
   SSize = 7 + NTotal*NFieldWidth;
   MultiLayerPerceptron[Network,StructInfo,0,0] = SSize;    // - SSize - величина обучаемого массива (кол-во опытов)
   MultiLayerPerceptron[Network,StructInfo,1,0] = NIn;      // - NIn - число входных данных
   MultiLayerPerceptron[Network,StructInfo,2,0] = NOut;     // - NOut - число выходных данных
   MultiLayerPerceptron[Network,StructInfo,3,0] = NTotal;   // - NTotal - 
   MultiLayerPerceptron[Network,StructInfo,4,0] = WCount;   // - WCount - число весовых/пороговых коэффициентов
   MultiLayerPerceptron[Network,StructInfo,5,0] = 7.0;      // - IStart - 
   MultiLayerPerceptron[Network,StructInfo,6,0] = 0.0;      // - IStart - 
//--------------------------------------------
   if( IsClsNet )
      MultiLayerPerceptron[Network,StructInfo,6,0] = 1.0;
   else
      MultiLayerPerceptron[Network,StructInfo,6,0] = 0.0;
//--------------------------------------------
// Заполните структуру: глобальная информация
   NProcessed = 0.0;
   WAllocated = 0.0;
   for( i=0; i<LayersCount; i++){
      for( j=0; j<LSizes[i]; j++){
         Offs = MultiLayerPerceptron[Network,StructInfo,5,0] + NProcessed*NFieldWidth;
         MultiLayerPerceptron[Network,StructInfo,(Offs+0),0] = LTypes[i];
         if( LTypes[i]==0 ) {
//--------------------------------------------
// Adaptive summator:
// * connections with weights to previous neurons
            MultiLayerPerceptron[Network,StructInfo,(Offs+1),0] = LNSyn[i];
            MultiLayerPerceptron[Network,StructInfo,(Offs+2),0] = LNFirst[LConnFirst[i]];
            MultiLayerPerceptron[Network,StructInfo,(Offs+3),0] = WAllocated;
            WAllocated = WAllocated + LNSyn[i];
            NProcessed = NProcessed + 1.0;
         }  // if
//--------------------------------------------
         if( LTypes[i]>0 ) {
// Адаптивный сумматор:
// * каждый нейрон, связанный с одним (только один) предыдущих нейронов.
// * никакие веса
            MultiLayerPerceptron[Network,StructInfo,(Offs+1),0] = 1.0;
            MultiLayerPerceptron[Network,StructInfo,(Offs+2),0] = LNFirst[LConnFirst[i]] + j;
            MultiLayerPerceptron[Network,StructInfo,(Offs+3),0] = -1.0;
            NProcessed = NProcessed + 1.0;
         }  // if
//--------------------------------------------
            if( LTypes[i]==-2 || LTypes[i]==-3 || LTypes[i]==-4 )
                NProcessed = NProcessed + 1.0;
      } // for J
   } // for I
//--------------------------------------------
// Заполните веса маленькими случайными ценностями
// Калибруйте средства и сигмы
   MathSrand(1);
   for( i=0; i<WCount; i++){
      MultiLayerPerceptron[Network,Weights,i,0] = 2.0*(RandomReal()) - 1.0;
}
//--------------------------------------------
   for( i=0; i<(NIn+NOut); i++){
      MultiLayerPerceptron[Network,ColumnMeans,i,0] = 0.0;
      MultiLayerPerceptron[Network,ColumnSigmas,i,0] = 1.0;
    } // for I
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Сериализация структуры MultiLayerPerceptron                                 |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      Network -   сеть-оригинал                                               |
//|                                                                              |
//|  Выходные параметры:                                                         |
//|      IA      -   массив целых чисел, хранящий информацию о структуре сети,   |
//|                  array [0..ILen-1]                                           |
//|      RA      -   массив вещественных чисел, хранящий информацию о весовых    |
//|                  коэффициентах сети, array [0..RLen-1]                       |
//|      ILen    -   длина массива целых чисел                                   |
//|      RLen    -   длина массива вещественных чисел                            |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 29.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPSerialize( int Network, int& IA[], double& RA[], int& ILen, int& RLen){
//--------------------------------------------
   int I, SSize, NTotal, NIn, NOut, WCount, i_, i1_;
//--------------------------------------------
// Выгрузка информации
//--------------------------------------------
   SSize = MultiLayerPerceptron[Network,StructInfo,0,0];    // - SSize - величина обучаемого массива (кол-во опытов)
   NIn = MultiLayerPerceptron[Network,StructInfo,1,0];      // - NIn - число входных данных
   NOut = MultiLayerPerceptron[Network,StructInfo,2,0];     // - NOut - число выходных данных
   NTotal = MultiLayerPerceptron[Network,StructInfo,3,0];   // - NTotal - 
   WCount = MultiLayerPerceptron[Network,StructInfo,4,0];   // - WCount - число весовых/пороговых коэффициентов
//--------------------------------------------
// IA формат:
//      IDX         DESCR.
//      0           версия (1 - текущая версия).
//      1           размер инфотмации о структуре НС;
//      2-...       инфотмация о структуре НС;
//--------------------------------------------
//  RA формат:
//      LEN         DESRC.
//      WCount      Веса;
//      NIn+NOut    Средства Колонки;
//      NIn+NOut    Сигмы Колонки;
//--------------------------------------------
   ILen = 2+SSize;
   RLen = (WCount+2*(NIn+NOut));
   ArrayResize(IA,ILen);
   ArrayResize(RA,RLen);
   IA[0] = 1.0;
   IA[1] = SSize;
//--------------------------------------------
   for( int i=0; i<SSize; i++)
      IA[2+i] = MultiLayerPerceptron[Network,StructInfo,i,0];           // Network.StructInfo(I)
//--------------------------------------------
    for( i=0; i<WCount; i++)
        RA[i] = MultiLayerPerceptron[Network,Weights,i,0];              // Network.Weights(i_)
//--------------------------------------------
    int j = (0) - (WCount);
    for( i=WCount; i<(WCount+NIn+NOut); i++)
        RA[i] = MultiLayerPerceptron[Network,ColumnMeans,(i+j),0];      // Network.ColumnMeans(i+j)
//--------------------------------------------
    j = (0) - (WCount+NIn+NOut);
    for( i=(WCount+NIn+NOut); i<(WCount+2*(NIn+NOut)); i++)
        RA[i] = MultiLayerPerceptron[Network,ColumnSigmas,(i+j),0];      // Network.ColumnSigmas(i+j)
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Де-сериализация структуры MultiLayerPerceptron                              |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      IA      -   массив целых чисел, хранящий информацию о структуре сети,   |
//|                  array [0..ILen-1]                                           |
//|      RA      -   массив вещественных чисел, хранящий информацию о весовых    |
//|                  коэффициентах сети, array [0..RLen-1]                       |
//|                                                                              |
//|  Выходные параметры:                                                         |
//|      Network -   сеть-оригинал                                               |
//|                                                                              |
//|  Результат:                                                                  |
//|      True, если сеть успешно восстановлена.                                  |
//|      False, если произошли ошибки (например, версия нейронной сети не        |
//|  поддерживается текущей версией библиотеки).                                 |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 29.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

bool MLPUnserialize( int& IA[], double& RA[], int Network ){
//--------------------------------------------
   bool Result;
   int SSize, NTotal, NIn, NOut, WCount;
//--------------------------------------------
   if( IA[0] != 1 ){
      Result = False;
      return(False);
    }
//--------------------------------------------
   Result = True;
//--------------------------------------------
// Загрузка StructInfo из IA
    SSize = IA[1];
//    ReDim Network.StructInfo(0 To SSize-1)
    for( int i=0; i<SSize; i++)
        MultiLayerPerceptron[Network,StructInfo,i,0] = IA[2+i];       // Network.StructInfo(I) = IA(2+I)
//--------------------------------------------
// Загрузка информации из StructInfo
   SSize = MultiLayerPerceptron[Network,StructInfo,0,0];    // - SSize - величина обучаемого массива (кол-во опытов)
   NIn = MultiLayerPerceptron[Network,StructInfo,1,0];      // - NIn - число входных данных
   NOut = MultiLayerPerceptron[Network,StructInfo,2,0];     // - NOut - число выходных данных
   NTotal = MultiLayerPerceptron[Network,StructInfo,3,0];   // - NTotal - 
   WCount = MultiLayerPerceptron[Network,StructInfo,4,0];   // - WCount - число весовых/пороговых коэффициентов
//--------------------------------------------
// Копирование параметров из RA
   for( i=0; i<WCount; i++)
      MultiLayerPerceptron[Network,Weights,i,0] = RA[i];             // Network.Weights(i_) = RA(i_)
//--------------------------------------------
   int j = (WCount) - (0);
   for( i=0; i<(NIn+NOut); i++)
      MultiLayerPerceptron[Network,ColumnMeans,i,0] = RA[i+j];   // Network.ColumnMeans(i_) = RA(i_+i1_)
//--------------------------------------------
   j = (WCount+NIn+NOut) - (0);
   for(i=0; i<(NIn+NOut); i++)
      MultiLayerPerceptron[Network,ColumnSigmas,i,0] = RA[i+j];  // Network.ColumnSigmas(i_) = RA(i_+i1_)
//--------------------------------------------
   return(Result);
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Рандомизация MultiLayerPerceptron без изменения структуры связей.           |
//|                                                                              |
//|  Подпрограмма заполняет веса и стандартизатор малыми случайными значениями.  |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      Network         -   инициализированная обученная/необученная сеть       |
//|                                                                              |
//|  Выходные параметры:                                                         |
//|      Network         -   рандомизированная сеть                              |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 10.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPRandomizeFull( int& Network ){
//--------------------------------------------
   int i, NIn, NOut, WCount, NTotal, IStart, Offs, NType;
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);
   NTotal = MultiLayerPerceptron[Network,StructInfo,3,0];      // Network.StructInfo(3)
   IStart = MultiLayerPerceptron[Network,StructInfo,5,0];      // Network.StructInfo(5)
//--------------------------------------------
// Процесс нейросети
//--------------------------------------------
   MathSrand(1);
   for( i=0; i<WCount; i++)
      MultiLayerPerceptron[Network,Weights,i,0] = 2.0*RandomReal() - 1.0;   // Network.Weights(I) = 2*Rnd()-1
//--------------------------------------------
   MathSrand(1);
   for( i=0; i<NIn; i++){
      MultiLayerPerceptron[Network,ColumnMeans,i,0] = 2.0*RandomReal() - 1.0;         // Network.ColumnMeans(I) = 2*Rnd()-1
      MultiLayerPerceptron[Network,ColumnSigmas,i,0] = 1.5*RandomReal() + 0.5;      // Network.ColumnSigmas(I) = 1.5*Rnd()+0.5
   } // for I
//--------------------------------------------
   MathSrand(1);
   for( i=0; i<NOut; i++){
      Offs = IStart+(NTotal-NOut+i)*NFieldWidth;
      NType = MultiLayerPerceptron[Network,StructInfo,(Offs+0),0];   // Network.StructInfo(Offs+0);
//--------------------------------------------
      if( NType==0 )
// Изменения только для линейных нейронов
         MultiLayerPerceptron[Network,ColumnMeans,(NIn+i),0] = 2.0*RandomReal() - 1.0;   // Network.ColumnMeans(NIn+I) = 2*Rnd()-1
//--------------------------------------------
      if( NType==0 || NType==3 )
// Весы изменены только для линейных или ограниченных нейронов продукций.
// Отметьте, что рандомизация масштаба сохраняет признак.
         MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0] = Sgn(MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0])*(1.5*RandomReal() + 0.5);    // Network.ColumnSigmas(NIn+I) = Sgn(Network.ColumnSigmas(NIn+I))*(1.5*Rnd()+0.5)
//--------------------------------------------
    } // for I
//-----
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Информация о типе нормализации.                                             |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

bool MLPIsSoftmax( int Network ){
//--------------------------------------------
   bool Result;
//--------------------------------------------
   Result = MultiLayerPerceptron[Network,StructInfo,6,0]==1.0;      // Network.StructInfo(6)=1
//--------------------------------------------
   return(Result);
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Получение среднеквадратичной ошибки на тестовом множестве                   |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      Network -   структура, задающая обученную/необученную нейронную  сеть.  |
//|                  структура передается по ссылке  и  модифицируется  в  ходе  |
//|                  работы (т.к. её поля используются для  хранения  временных  |
//|                  переменных).                                                |
//|      XY      -   тестовое множество.                                         |
//|                  массив размером [0..SSize-1,0..NIn+NOut-1].                 |
//|                  первые   NIn   столбцов   содержат   входные   данные,      |
//|                  последующие NOut столбцов  содержат желаемые  значения      |
//|                  выходов сети.                                               |
//|      SSize   -   размер тестового множества                                  |
//|                                                                              |
//|  Результат:                                                                  |
//|      среднеквадратичная ошибка                                               |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

double MLPRMSError( int Network, double& XY[][], int& SSize ){
//--------------------------------------------
   int NIn, NOut, WCount;
   double Result;
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);
   Result = MathSqrt(2*MLPError(Network, XY, SSize)/(SSize*NOut));
//--------------------------------------------
   return(Result);
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Получение среднего значения модуля ошибки на тестовом множестве             |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      Network -   структура, задающая обученную/необученную нейронную  сеть.  |
//|                  структура передается по ссылке  и  модифицируется  в  ходе  |
//|                  работы (т.к. её поля используются для  хранения  временных  |
//|                  переменных).                                                |
//|      XY      -   тестовое множество.                                         |
//|                  массив размером [0..SSize-1,0..NIn+NOut-1].                 |
//|                  первые   NIn   столбцов   содержат   входные   данные,      |
//|                  последующие NOut столбцов  содержат желаемые  значения      |
//|                  выходов сети.                                               |
//|      SSize   -   размер тестового множества                                  |
//|                                                                              |
//|  Результат:                                                                  |
//|      SUM(|Yi-NETi|)/N                                                        |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 11.03.2008 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

double MLPAvgError( int Network, double& XY[][], int SSize ){
//--------------------------------------------
   int i, j, NIn, NOut, WCount;
   double Result, X[], Y[];
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);
   Result = 0.0;
   ArrayResize(X,NIn);
   ArrayResize(Y,NOut);
//--------------------------------------------
   for( i=0; i<SSize; i++){
      for( j=0; j<NIn; j++){
         MultiLayerPerceptron[Network,MLP_X,j,0] = XY[i,j];      // Network.X(j) = XY(i,j)
         X[j] = (XY[i,j]);
      }
//--------------------------------------------
      for( j=NIn; j<(NIn+NOut); j++)
         Y[j-NIn] = (XY[i,j]);
//--------------------------------------------
      MLPProcess(Network,X,Y);
//--------------------------------------------
      for( j=0; j<NOut; j++)
         Result += MathAbs(XY[i,(NIn+j)]-MultiLayerPerceptron[Network,MLP_Y,j,0]);   // Network.Y(j))
   } // for I
//--------------------------------------------
   Result /= (SSize*NOut);
   return(Result);
//----
}


//+---------------------------------------------------------------------------------+
//+---------------------------------------------------------------------------------+
//+---------------------------------------------------------------------------------+
//|  Получение среднего значения модуля относительной ошибки на тестовом множестве  |
//|                                                                                 |
//|  Входные параметры:                                                             |
//|      Network -   структура, задающая обученную/необученную нейронную  сеть.     |
//|                  структура передается по ссылке  и  модифицируется  в  ходе     |
//|                  работы (т.к. её поля используются для  хранения  временных     |
//|                  переменных).                                                   |
//|      XY      -   тестовое множество.                                            |
//|                  массив размером [0..SSize-1,0..NIn+NOut-1].                    |
//|                  первые   NIn   столбцов   содержат   входные   данные,         |
//|                  последующие NOut столбцов  содержат желаемые  значения         |
//|                  выходов сети.                                                  |
//|      SSize   -   размер тестового множества                                     |
//|                                                                                 |
//|  Результат:                                                                     |
//|      SUM(|Yi-NETi|/|Yi|)/N, сумма ведется по Yi<>0.                             |
//|                                                                                 |
//|    -- ALGLIB --                                                                 |
//|       Copyright 11.03.2008 by Bochkanov Sergey                                  |
//+------------------------------------------------------------------------------+

double MLPAvgRelError( int Network, double& XY[][], int SSize ){
//--------------------------------------------
   int i, j, K, NIn, NOut, WCount, i_;
   double Result, X[], Y[];
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);
   ArrayResize(X,NIn);
   ArrayResize(Y,NOut);
   Result = 0.0;
   K = 0.0;
//--------------------------------------------
   for( i=0; i<SSize; i++){
      for( j=0; j<NIn; j++){
         MultiLayerPerceptron[Network,MLP_X,j,0] = XY[i,j];      // Network.X(j) = XY(i,j)
         X[j] = (XY[i,j]);
      }
//--------------------------------------------
      for( j=0; j<NOut; j++)
         Y[j] = (XY[i,(j+NIn)]);
//--------------------------------------------
      MLPProcess(Network,X,Y);
//--------------------------------------------
      for( j=0; j<NOut; j++)
         if( XY[i,(NIn+j)] != 0 ) {
            Result = Result+MathAbs(XY[i,(NIn+j)] - MultiLayerPerceptron[Network,MLP_Y,j,0])/MathAbs(XY[i,(NIn+j)]);
            K = K+1.0;
         }
//--------------------------------------------
   } // for I
//--------------------------------------------
   if( K != 0 ){
      Result /= K;
   }
//--------------------------------------------
    return(Result);
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Вычисление гессиана  функции ошибки на основе пакета образов, используется  |
//|  естественная функция ошибки  (сумма  квадратов  для  задач  аппроксимации,  |
//|  кросс-энтропия - для задач классификации).                                  |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      Network -   структура, задающая обученную/необученную нейронную  сеть.  |
//|                  структура передается по ссылке  и  модифицируется  в  ходе  |
//|                  работы (т.к. её поля используются для  хранения  временных  |
//|                  переменных).                                                |
//|      XY      -   пакет образов.                                              |
//|                  массив размером [0..SSize-1,0..NIn+NOut-1].                 |
//|                  первые   NIn   столбцов   содержат   входные   данные,      |
//|                  последующие NOut столбцов  содержат желаемые  значения      |
//|                  выходов сети.                                               |
//|      SSize   -   размер пакета                                               |
//|                                                                              |
//|  Выходные параметры:                                                         |
//|      E       -   функция ошибки                                              |
//|      Grad    -   градиент функции ошибки по весовым/пороговым коэффициентам. |
//|                  Подпрограмма  не  выделяет  память  под  этот  вектор, это  |
//|                  обязанность   того,  кто  вызывает  подпрограмму, выделить  |
//|                  память под результат.                                       |
//|                  Массив должен иметь  размер [0..WCount-1].                  |
//|      H       -   гессиан функции ошибки по весовым/пороговым коэффициентам.  |
//|                  Подпрограмма  не  выделяет  память  под  этот  вектор, это  |
//|                  обязанность   того,  кто  вызывает  подпрограмму, выделить  |
//|                  память под результат.                                       |
//|                  Массив должен иметь  размер [0..WCount-1,0..WCount-1].      |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 26.01.2008 by Bochkanov Sergey.                              |
//|                                                                              |
//|       Hessian calculation based on R-algorithm described in                  |
//|       "Fast Exact Multiplication by the Hessian",                            |
//|       B. A. Pearlmutter,                                                     |
//|       Neural Computation, 1994.                                              |
//+------------------------------------------------------------------------------+

void MLPHessianNBatch( int Network, double& XY[][], int& SSize, double& E, double& Grad[], double& H[][] ){
//--------------------------------------------
   int i, j, k, i_, i1_, NIn, NOut, WCount, NTotal, IStart, Offs, N1, N2, W1, W2;
   double S, T, V, ET, BFlag, F, DF, D2F, dEIdYJ, MX, NET, X[], DesiredY[], GT[], Zeros[], RX[][250], RY[][250], RDX[][250], RDY[][250];
//--------------------------------------------
   MLPProperties(Network, NIn, NOut, WCount);
   NTotal = MultiLayerPerceptron[Network,StructInfo,3,0];      // NTotal = Network.StructInfo(3)
   IStart = MultiLayerPerceptron[Network,StructInfo,5,0];      // IStart = Network.StructInfo(5)
//--------------------------------------------
// Prepare
   ArrayResize(X,NIn);              // ReDim X(0 To NIn-1)
   ArrayResize(DesiredY,NOut);      // ReDim DesiredY(0 To NOut-1)
   ArrayResize(Zeros,WCount);       // ReDim Zeros(0 To WCount-1)
   ArrayResize(GT,WCount);          // ReDim GT(0 To WCount-1)
   ArrayResize(RX,(NTotal+NOut));   // ReDim RX(0 To NTotal+NOut-1, 0 To WCount-1)
   ArrayResize(RY,(NTotal+NOut));   // ReDim RY(0 To NTotal+NOut-1, 0 To WCount-1)
   ArrayResize(RDX,(NTotal+NOut));  // ReDim RDX(0 To NTotal+NOut-1, 0 To WCount-1)
   ArrayResize(RDY,(NTotal+NOut));  // ReDim RDY(0 To NTotal+NOut-1, 0 To WCount-1)
//--------------------------------------------
   E = 0.0;
   ArrayInitialize(Zeros,0.0);
//--------------------------------------------
   for( i=0; i<WCount; i++)
      Grad[i] = Zeros[i];
//--------------------------------------------
   for( i=0; i<WCount; i++)
      for( j=0; j<WCount; j++)
         H[i,j] = Zeros[j];
//--------------------------------------------
// Process
   for( k=0; k<SSize; k++){
//--------------------------------------------
// Обработайте вектор с MLPGradN.
// Теперь Нейроны, DFDNET и DError содержат результаты последнего пробега.
      for( i=0; i<NIn; i++)
         X[i] = XY[k,i];
//--------------------------------------------
      i1_ = (NIn) - (0);
      for( i=0; i<NOut; i++)
         DesiredY[i] = XY[k,i+i1_];
//--------------------------------------------
      MLPGradN(Network, X, DesiredY, ET, GT);
//--------------------------------------------
// градиент, ошибка
      E += ET;
      for( i=0; i<WCount; i++)
         Grad[i] += GT[i];
//--------------------------------------------
// Мешковина. Отправьте проход R-алгоритма
      for( i=0; i<NTotal; i++){
         Offs = IStart+i*NFieldWidth;
//--------------------------------------------
         for( i_=0; i_<WCount; i_++)
            RX[i,i_] = Zeros[i_];
//--------------------------------------------
         for( i_=0; i_<WCount; i_++)
            RY[i,i_] = Zeros[i_];
//--------------------------------------------
         if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]>0 ){
//--------------------------------------------
// Активация функции
            N1 = MultiLayerPerceptron[Network,StructInfo,(Offs+2),0];
            for( i_=0; i_<WCount; i_++)
               RX[i,i_] = RY[N1,i_];
//--------------------------------------------
            V = MultiLayerPerceptron[Network,DFDNET,i,0];
            for( i_=0; i_<WCount; i_++)
               RY[i,i_] = V*RX[i,i_];
         } // if
//--------------------------------------------
         if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]==0 ){
// Адаптивный summator
            N1 = MultiLayerPerceptron[Network,StructInfo,(Offs+2),0];
            N2 = N1 + MultiLayerPerceptron[Network,StructInfo,(Offs+1),0] - 1.0;
            W1 = MultiLayerPerceptron[Network,StructInfo,(Offs+3),0];
            W2 = W1 + MultiLayerPerceptron[Network,StructInfo,(Offs+1),0] - 1.0;
//--------------------------------------------
            for( j=N1; j<=N2; j++){
               V = MultiLayerPerceptron[Network,Weights,(W1+j-N1),0];
//--------------------------------------------
               for( i_=0; i_<WCount; i_++)
                  RX[i,i_] = RX[i,i_] + V*RY[j,i_];
//--------------------------------------------
               RX[i,(W1+j-N1)] += MultiLayerPerceptron[Network,Neurons,j,0];
            } // for J
//--------------------------------------------
            for( i_=0; i_<WCount; i_++)
               RY[i,i_] = RX[i,i_];
         }  // if
//--------------------------------------------
         if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]<0 ){
            BFlag = True;
//--------------------------------------------
// нейрон входа, оставленный неизменный  //input neuron, left unchanged
            if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]==-2 )
               BFlag = False;
//--------------------------------------------
// "-1" нейрон, оставленный неизменный  // "-1" neuron, left unchanged
            if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]==-3 )
               BFlag = False;
//--------------------------------------------
// "0" нейрон, оставленный неизменный  // "0" neuron, left unchanged
            if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]==-4 )
               BFlag = False;
         }  // if
//--------------------------------------------
      } // I
//--------------------------------------------
// Мешковина. Обратный проход R-алгоритма.  // Hessian. Backward pass of the R-algorithm.
// Стадия 1. Калибруйте RDY   // Stage 1. Initialize RDY
      for( i=0; i<(NTotal+NOut); i++)
         for( i_=0; i_<WCount; i_++)
            RDY[i,i_] = Zeros[i_];
//--------------------------------------------
      if( MultiLayerPerceptron[Network,StructInfo,6,0]==0 ){
//--------------------------------------------
//  Стандартизация.
            
//  В контексте вычисления Мешковины стандартизацию рассматривают 
//  как дополнительный слой с невесомой функцией активации:
            
//  F(NET) := Sigma*NET
           
//  Таким образом мы добавляем еще один слой, чтобы отправить проход, 
//  и сделать передовым / обратный проход через этот слой.
//--------------------------------------------
         for( i=0; i<NOut; i++){
            N1 = NTotal-NOut+i;
            N2 = NTotal+i;
//--------------------------------------------
// Отправьте проход от N1 до N2
            for( i_=0; i_<WCount; i_++)
               RX[N2,i_] = RY[N1,i_];
//--------------------------------------------
            V = MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0];
            for( i_=0; i_<WCount; i_++)
               RY[N2,i_] = V*RX[N2,i_];
//--------------------------------------------
// Инициализация RDY
            for( i_=0; i_<WCount; i_++)
               RDY[N2,i_] = RY[N2,i_];
//--------------------------------------------
//  Обратный проход от N2 до N1:
//  1. Вычислить R (dE/dX).
//  2. Никакой R (dE/dWij) не необходим, так как вес нейрона активации 
//  установлен к 1. Таким образом мы можем обновить R (dE/dY) 
//  для связанного нейрона (отметьте что Vij=0, Wij=1), 
//--------------------------------------------
            DF = MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0];
            for( i_=0; i_<WCount; i_++)
               RDX[N2,i_] = DF*RDY[N2,i_];
//--------------------------------------------
            for( i_=0; i_<WCount; i_++)
               RDY[N1,i_] = RDY[N1,i_] + RDX[N2,i_];
         } // for I
      } else {
//--------------------------------------------
//  Softmax.
                   
//  Калибруйте RDY использование обобщенного выражения для ei 
//  (пошел) (см. выражение (9) от p. 5 из "Быстро Точное Умножение Мешковиной").
            
//  Когда мы работаем с softmax сетью, обобщенное выражение для ei' (yi)
//  используется, так как комбинация ошибочной функции поперечной энтропии 
//  и softmax нормализации приводит к ei, который зависит от всего y's 
//  (для ошибочной функции наименьших квадратов, ei зависит только от пошел).
            
//  Так, для a
            
//  S = sum(exp(yk)),
//  ei = sum(trn)*exp(yi)/S
            
//  мы имеем
            
//  j=i:   d(ei)/d(yj) = T*exp(yi)*(S-exp(yi))/S^2
//  j<>i:  d(ei)/d(yj) = -T*exp(yi)*exp(yj)/S^2
//--------------------------------------------
         T = 0.0;
         for( i=0; i<NOut; i++)
            T += DesiredY[i];
//--------------------------------------------
         MX = MultiLayerPerceptron[Network,Neurons,(NTotal-NOut),0];
         for( i=0; i<NOut; i++)
            MX = MathMax(MX, MultiLayerPerceptron[Network,Neurons,(NTotal-NOut+i),0]);
//--------------------------------------------
         S = 0.0;
         for( i=0; i<NOut; i++){
            MultiLayerPerceptron[Network,NWBuf,i,0] = MathExp(MultiLayerPerceptron[Network,Neurons,(NTotal-NOut+i),0] - MX);
            S += MultiLayerPerceptron[Network,NWBuf,i,0];
         } // for I
//--------------------------------------------
         for( i=0; i<NOut; i++){
//--------------------------------------------
            for( j=0; j<NOut; j++){
//--------------------------------------------
               if( j==i ) {
                  dEIdYJ = T*MultiLayerPerceptron[Network,NWBuf,i,0]*(S-MultiLayerPerceptron[Network,NWBuf,i,0])/MathPow(S,2);
//--------------------------------------------
                  for( i_=0; i_<WCount; i_++)
                     RDY[(NTotal-NOut+i),i_] += dEIdYJ*RY[(NTotal-NOut+i),i_];
//--------------------------------------------
               } else {
                  dEIdYJ = -(T*MultiLayerPerceptron[Network,NWBuf,i,0]*MultiLayerPerceptron[Network,NWBuf,j,0]/MathPow(S,2));
//--------------------------------------------
                  for( i_=0; i_<WCount; i_++)
                     RDY[(NTotal-NOut+i),i_] = RDY[(NTotal-NOut+i),i_] + dEIdYJ*RY[(NTotal-NOut+j),i_];
//--------------------------------------------
               }
//--------------------------------------------
            } // J
//--------------------------------------------
         } // I
//--------------------------------------------
      }
//--------------------------------------------
// Мешковина. Обратный проход R-алгоритма
// Стадия 2. Process.
      for( i=NTotal-1; i>=0; i--){
//--------------------------------------------
//    Возможные варианты: 
//    1. Функция активации 
//    2. Адаптивный summator 
//    3. Специальный нейрон
//--------------------------------------------
         Offs = IStart+i*NFieldWidth;
         if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]>0 ) {
            N1 = MultiLayerPerceptron[Network,StructInfo,(Offs+2),0];
//--------------------------------------------
// Сначала, вычислите R (dE/dX).
            MLPActivationFunction(MultiLayerPerceptron[Network,Neurons,N1,0], 
                                       MultiLayerPerceptron[Network,StructInfo,Offs,0], F, DF, D2F);
//--------------------------------------------
            V = D2F*MultiLayerPerceptron[Network,DError,i,0];
            for( i_=0; i_<WCount; i_++)
               RDX[i,i_] = DF*RDY[i,i_];
//--------------------------------------------
            for( i_=0; i_<WCount; i_++)
               RDX[i,i_] = RDX[i,i_] + V*RX[i,i_];
//--------------------------------------------
//  Никакой R (dE/dWij) не необходим, так как вес нейрона активации установлен к 1. 
//  Таким образом мы можем обновить R (dE/dY) для связанного нейрона. (отметьте что Vij=0, Wij=1), 
//--------------------------------------------
            for( i_=0; i_<WCount; i_++)
               RDY[N1,i_] = RDY[N1,i_] + RDX[i,i_];
         }  // if
//--------------------------------------------
         if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]==0 ) {
// Адаптивный сумматор
            N1 = MultiLayerPerceptron[Network,StructInfo,(Offs+2),0];
            N2 = N1 + MultiLayerPerceptron[Network,StructInfo,(Offs+1),0] - 1.0;
            W1 = MultiLayerPerceptron[Network,StructInfo,(Offs+3),0];
            W2 = W1 + MultiLayerPerceptron[Network,StructInfo,(Offs+1),0] - 1.0;
//--------------------------------------------
// Сначала, вычислите R (dE/dX).
            for( i_=0; i_<WCount; i_++)
               RDX[i,i_] = RDY[i,i_];
//--------------------------------------------
// Тогда, вычислите R (dE/dWij)
            for( j=W1; j<=W2; j++){
               V = MultiLayerPerceptron[Network,Neurons,(N1+j-W1),0];
//--------------------------------------------
               for( i_=0; i_<WCount; i_++)
                  H[j,i_] = H[j,i_] + V*RDX[i,i_];
//--------------------------------------------
               V = MultiLayerPerceptron[Network,DError,i,0];
               for( i_=0; i_<WCount; i_++)
                  H[j,i_] += V*RY[(N1+j-W1),i_];
            } // J
//--------------------------------------------
// И наконец, обновление R (dE/dY) для связанных нейронов.
            for( j=W1; j<=W2; j++){
               V = MultiLayerPerceptron[Network,Weights,j,0];
//--------------------------------------------
               for( i_=0; i_<WCount; i_++)
                  RDY[(N1+j-W1),i_] = RDY[(N1+j-W1),i_] + V*RDX[i,i_];
//--------------------------------------------
               RDY[(N1+j-W1),j] = RDY[(N1+j-W1),j] + MultiLayerPerceptron[Network,DError,i,0];
            } // J
//--------------------------------------------
         }  // if
//--------------------------------------------
         if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]<0 ){
            BFlag = False;
//--------------------------------------------
            if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]==-2 || 
                     MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]==-3 || 
                           MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]==-4 )
// Специальный тип нейрона, никакая требуемая обратная связь
               BFlag = True;
//--------------------------------------------
         }
//--------------------------------------------
      } // for I
//--------------------------------------------
   } // for K
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Вычисление градиента функции ошибки на основе входа и желаемого выхода.     |
//|  Функция ошибки - наименьшие квадраты.                                       |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      Network -   структура, задающая обученную/необученную нейронную  сеть.  |
//|                  структура передается по ссылке  и  модифицируется  в  ходе  |
//|                  работы (т.к. её поля используются для  хранения  временных  |
//|                  переменных).                                                |
//|      X       -   входной вектор,  по размеру должен быть равен числу входов  |
//|                  сети, [0..NIn-1].                                           |
//|      DesiredY-   выходной вектор, по размеру должен быть равен числу выходов |
//|                  сети, [0..NOut-1].                                          |
//|                                                                              |
//|  Выходные параметры:                                                         |
//|      E       -   функция ошибки, SUM(sqr(y[i]-desy[i])/2,i)                  |
//|      Grad    -   градиент функции ошибки по весовым/пороговым коэффициентам. |
//|                  Подпрограмма  не  выделяет  память  под  этот  вектор, это  |
//|                  обязанность   того,  кто  вызывает  подпрограмму, выделить  |
//|                  память под результат.                                       |
//|                  Массив должен иметь  размер [0..WCount-1].                  |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPGrad( int Network, double& X[], double& DesiredY[], double& E, double& Grad[] ){
//--------------------------------------------
   int i, NOut, NTotal, WCount;
   double Y[], Neurons_[], Weights_[], DError_[];
//--------------------------------------------
// Подготовите dError/dOut, внутренние структуры
   NOut = MultiLayerPerceptron[Network,StructInfo,2,0];                   // NOut = Network.StructInfo(2)
   NTotal = MultiLayerPerceptron[Network,StructInfo,3,0];                 // NTotal = Network.StructInfo(3)
   WCount = MultiLayerPerceptron[Network,StructInfo,4,0];
   ArrayResize(Y,NOut);
   ArrayResize(Neurons_,WCount);
   ArrayResize(Weights_,WCount);
   ArrayResize(DError_,WCount);
//--------------------------------------------
   for( i=0; i<NOut; i++)
      Y[i] = MultiLayerPerceptron[Network,MLP_Y,i,0];
//--------------------------------------------
   MLPProcess(Network,X,Y);
//--------------------------------------------
   for( i=0; i<NOut; i++)
      MultiLayerPerceptron[Network,MLP_Y,i,0] = Y[i];
//--------------------------------------------
   E = 0.0;
//--------------------------------------------
   for( i=0; i<NTotal; i++)
      MultiLayerPerceptron[Network,DError,i,0] = 0.0;
//--------------------------------------------
   for( i=0; i<NOut; i++){
      MultiLayerPerceptron[Network,DError,(NTotal-NOut+i),0] = 
            MultiLayerPerceptron[Network,MLP_Y,i,0]-DesiredY[i];   // Network.DError(NTotal-NOut+I) = Network.Y(I)-DesiredY[i];
      E += MathPow((MultiLayerPerceptron[Network,MLP_Y,i,0]-DesiredY[i]),2)/2.0;                   // E = E+Square(Network.Y(I)-DesiredY(I))/2
   } // for I
    
//--------------------------------------------
// градиент
   for( i=0; i<WCount; i++){
      Neurons_[i] = MultiLayerPerceptron[Network,Neurons,i,0];
      Weights_[i] = MultiLayerPerceptron[Network,Weights,i,0];
      DError_[i] = MultiLayerPerceptron[Network,DError,i,0];
   }
   MLPInternalCalculateGradient(Network, Neurons_, Weights_, DError_, Grad, False);
   for( i=0; i<WCount; i++){
      MultiLayerPerceptron[Network,Neurons,i,0] = Neurons_[i];
      MultiLayerPerceptron[Network,Weights,i,0] = Weights_[i];
      MultiLayerPerceptron[Network,DError,i,0] = DError_[i];
   }
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|  Вычисление градиента функции ошибки на основе входа и желаемого выхода.     |
//|  Функция ошибки - естественная.                                              |
//|                                                                              |
//|  Входные параметры:                                                          |
//|      Network -   структура, задающая обученную/необученную нейронную  сеть.  |
//|                  структура передается по ссылке  и  модифицируется  в  ходе  |
//|                  работы (т.к. её поля используются для  хранения  временных  |
//|                  переменных).                                                |
//|      X       -   входной вектор,  по размеру должен быть равен числу входов  |
//|                  сети, [0..NIn-1].                                           |
//|      DesiredY-   выходной вектор, по размеру должен быть равен числу выходов |
//|                  сети, [0..NOut-1].                                          |
//|                                                                              |
//|  Выходные параметры:                                                         |
//|      E       -   функция ошибки (наименьшие  квадраты  для сети-регрессора,  |
//|                  кросс-энтропия для сети-классификатора).                    |
//|      Grad    -   градиент функции ошибки по весовым/пороговым коэффициентам. |
//|                  Подпрограмма  не  выделяет  память  под  этот  вектор, это  |
//|                  обязанность   того,  кто  вызывает  подпрограмму, выделить  |
//|                  память под результат.                                       |
//|                  Массив должен иметь  размер [0..WCount-1].                  |
//|                                                                              |
//|    -- ALGLIB --                                                              |
//|       Copyright 04.11.2007 by Bochkanov Sergey                               |
//+------------------------------------------------------------------------------+

void MLPGradN( int Network, double& X[], double& DesiredY[], double& E, double& Grad[] ){
//--------------------------------------------
   int i, NIn, NOut, NTotal, WCount;
   double S, Y[], Neurons_[], Weights_[], DError_[];
//--------------------------------------------
// Подготовите dError/dOut, внутренние структуры
   NOut = MultiLayerPerceptron[Network,StructInfo,2,0];
   NTotal = MultiLayerPerceptron[Network,StructInfo,3,0];
   WCount = MultiLayerPerceptron[Network,StructInfo,4,0];
//--------------------------------------------
   ArrayResize(Y,NOut);
   ArrayResize(Neurons_,WCount);
   ArrayResize(Weights_,WCount);
   ArrayResize(DError_,WCount);
//--------------------------------------------
   for( i=0; i<NOut; i++)
      Y[i] = MultiLayerPerceptron[Network,MLP_Y,i,0];
//--------------------------------------------
   MLPProcess(Network,X,Y);
   for( i=0; i<NOut; i++)
      MultiLayerPerceptron[Network,MLP_Y,i,0] = Y[i];
//--------------------------------------------
   for( i=0; i<NTotal; i++)
      MultiLayerPerceptron[Network,DError,i,0] = 0.0;
//--------------------------------------------
// Сеть регресса, наименьшие квадраты
   E = 0.0;
   if( MultiLayerPerceptron[Network,StructInfo,6,0]==0 )
//--------------------------------------------
      for( i=0; i<NOut; i++){
         MultiLayerPerceptron[Network,DError,(NTotal-NOut+i),0] = MultiLayerPerceptron[Network,MLP_Y,i,0]-DesiredY[i];
         E += MathPow((MultiLayerPerceptron[Network,MLP_Y,i,0]-DesiredY[i]),2)/2.0;
      } // for I
//--------------------------------------------
   else {
//--------------------------------------------
// Сеть классификации, поперечная энтропия
      S = 0.0;
      for( i=0; i<NOut; i++)
         S += DesiredY[i];
//--------------------------------------------
      for( i=0; i<NOut; i++){
         MultiLayerPerceptron[Network,DError,(NTotal-NOut+i),0] = S*MultiLayerPerceptron[Network,MLP_Y,i,0] - DesiredY[i];
         E += SafeCrossEntropy(DesiredY[i], MultiLayerPerceptron[Network,MLP_Y,i,0]);
      } // for I
   }
//--------------------------------------------
// градиент
   for( i=0; i<WCount; i++){
      Neurons_[i] = MultiLayerPerceptron[Network,Neurons,i,0];
      Weights_[i] = MultiLayerPerceptron[Network,Weights,i,0];
      DError_[i] = MultiLayerPerceptron[Network,DError,i,0];
   }
   MLPInternalCalculateGradient(Network, Neurons_, Weights_, DError_, Grad, True);
   for( i=0; i<WCount; i++){
      MultiLayerPerceptron[Network,Neurons,i,0] = Neurons_[i];
      MultiLayerPerceptron[Network,Weights,i,0] = Weights_[i];
      MultiLayerPerceptron[Network,DError,i,0] = DError_[i];
   }
//--------------------------------------------
}


//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//|                                                                              |
//|  Внутренняя подпрограмма                                                     |
//|  Сеть должна быть обработана MLPProcess на X                                 |
//|                                                                              |
//+------------------------------------------------------------------------------+

void MLPInternalCalculateGradient( int Network, double& Neurons_[], double& Weights[], 
                                    double& DError_[], double& Grad[], bool NaturalErrorFunc){
//--------------------------------------------
   int i_, i1_, i, j, N1, N2, W1, W2, NTotal, IStart, NIn, NOut, Offs, Ind;
   double dEdF, dFdNET, V, FOwn, DEOwn, NET, MX;
   bool BFlag;
//--------------------------------------------
// Чтение геометрии сети
   NIn = MultiLayerPerceptron[Network,StructInfo,1,0];
   NOut = MultiLayerPerceptron[Network,StructInfo,2,0];
   NTotal = MultiLayerPerceptron[Network,StructInfo,3,0];
   IStart = MultiLayerPerceptron[Network,StructInfo,5,0];
//--------------------------------------------
// Предварительная обработка dError/dOut:
// от dError/dOut (нормализованного) к (ненормализованному) dError/dOut
   if( MultiLayerPerceptron[Network,StructInfo,6,0]==1 ){
//--------------------------------------------
// Программа Мax
      if(  !(NaturalErrorFunc) ){
         MX = MultiLayerPerceptron[Network,Neurons,(NTotal-NOut),0];
//--------------------------------------------
         for( i=0; i<NOut; i++)
            MX = MathMax(MX, MultiLayerPerceptron[Network,Neurons,(NTotal-NOut+i),0]);
//--------------------------------------------
         NET = 0;
         for( i=0; i<NOut; i++){
            MultiLayerPerceptron[Network,NWBuf,i,0] = MathExp(MultiLayerPerceptron[Network,Neurons,(NTotal-NOut+i),0]-MX);
            NET += MultiLayerPerceptron[Network,NWBuf,i,0];
         } // for I
//--------------------------------------------
         i1_ = (0)-(NTotal-NOut);
         V = 0.0;
         for( i_=(NTotal-NOut); i_<NTotal; i_++)
            V += MultiLayerPerceptron[Network,DError,i_,0]*MultiLayerPerceptron[Network,NWBuf,(i_+i1_),0];
//--------------------------------------------
         for( i=0; i<NOut; i++){
            FOwn = MultiLayerPerceptron[Network,NWBuf,i,0];
            DEOwn = MultiLayerPerceptron[Network,DError,(NTotal-NOut+i),0];
            MultiLayerPerceptron[Network,NWBuf,(NOut+i),0] = (-V+DEOwn*FOwn+DEOwn*(NET-FOwn))*FOwn/MathPow(NET,2);
         } // for I
//--------------------------------------------
         for( i=0; i<NOut; i++)
            MultiLayerPerceptron[Network,DError,(NTotal-NOut+i),0] = MultiLayerPerceptron[Network,NWBuf,(NOut+i),0];
//--------------------------------------------
      }  // if
   } else
//--------------------------------------------
// Нестандартизация
      for( i=0; i<NOut; i++)
         MultiLayerPerceptron[Network,DError,(NTotal-NOut+i),0] = 
               MultiLayerPerceptron[Network,DError,(NTotal-NOut+i),0]*
                     MultiLayerPerceptron[Network,ColumnSigmas,(NIn+i),0];
//--------------------------------------------
// Обратная связь
   for( i=NTotal-1; i>=0; i--){
//--------------------------------------------
// Информация извлечения
      Offs = IStart+i*NFieldWidth;
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]>0 ){
//--------------------------------------------
// Активация функции
         dEdF = MultiLayerPerceptron[Network,DError,i,0];
         dFdNET = MultiLayerPerceptron[Network,DFDNET,i,0];
         Ind = (MultiLayerPerceptron[Network,StructInfo,(Offs+2),0]);
         DError_[Ind] += dEdF*dFdNET;
      }
//--------------------------------------------
// Адаптированный сумматор
      if( MultiLayerPerceptron[Network,StructInfo,(Offs+0),0]==0 ){
//--------------------------------------------
         N1 = (MultiLayerPerceptron[Network,StructInfo,(Offs+2),0]);
         N2 = N1 + (MultiLayerPerceptron[Network,StructInfo,(Offs+1),0]) - 1.0;
         W1 = (MultiLayerPerceptron[Network,StructInfo,(Offs+3),0]);
         W2 = W1 + (MultiLayerPerceptron[Network,StructInfo,(Offs+1),0]) - 1.0;
         dEdF = (MultiLayerPerceptron[Network,DError,i,0]);
         dFdNET = 1.0;
         V = dEdF*dFdNET;
         i1_ = (N1) - (W1);
//--------------------------------------------
         for( i_=W1; i_<=W2; i_++)
            Grad[i_] = V*Neurons_[i_+i1_];
//--------------------------------------------
         i1_ = (W1) - (N1);
         for( i_=N1; i_<=N2; i_++)
            DError_[i_] += V*Weights[(i_+i1_)];
      }
//--------------------------------------------
      if( MultiLayerPerceptron[Network,StructInfo,Offs,0]<0 ){
         BFlag = False;
//--------------------------------------------
         if( MultiLayerPerceptron[Network,StructInfo,Offs,0]==-2 ||
                  MultiLayerPerceptron[Network,StructInfo,Offs,0]==-3 ||
                        MultiLayerPerceptron[Network,StructInfo,Offs,0]==-4 )
// Специальный тип нейрона, никакая требуемая обратная связь
                BFlag = True;
//--------------------------------------------
      }
//--------------------------------------------
   } // I
//-----
}



//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+
//+------------------------------------------------------------------------------+








